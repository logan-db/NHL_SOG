# Bronze Layer Deployment Guide

## Overview

This guide walks through deploying and testing the new NHL API-based bronze layer in Databricks.

**Status**: Ready for Databricks deployment and testing

---

## Files Created

### Core Implementation
1. **`/src/dlt_etl/ingestion/01-bronze-ingestion-nhl-api.py`** (NEW)
   - Complete DLT pipeline using nhl-api-py
   - Four bronze tables with MoneyPuck-compatible schema
   - Configured quality checks and expectations

2. **`/src/utils/nhl_api_helper.py`** (NEW)
   - Parser functions for NHL API data
   - Situation classification, shot danger, team/player aggregation
   - 968 lines of well-documented code

3. **`/src/utils/test_nhl_api_validation.py`** (NEW)
   - 6 validation tests
   - 4/6 tests PASS (core functionality verified)

### Documentation
4. **`KNOWN_ISSUES.md`** (NEW)
   - Documents 2 minor edge case failures
   - Provides root cause analysis and fix recommendations
   - Test results summary (4/6 PASS)

5. **`MIGRATION_STATUS.md`** (UPDATED)
   - Progress tracking updated
   - Phase 2 marked complete

6. **`ZERO_DOWNSTREAM_CHANGES_STRATEGY.md`** (EXISTING)
   - Technical blueprint for schema preservation
   - Field-by-field calculation guide

---

## Bronze Tables Created

### Table Comparison: Old vs New

| Table Name | Old Source | New Source | Schema Changes |
|------------|------------|------------|----------------|
| `bronze_player_game_stats_v2` | MoneyPuck CSV | NHL API + Parser | **ZERO** - Exact match |
| `bronze_games_historical_v2` | MoneyPuck CSV | NHL API + Parser | **ZERO** - Exact match |
| `bronze_schedule_2023_v2` | Static table | NHL API (TODO) | No change yet (still static) |
| `bronze_skaters_2023_v2` | MoneyPuck CSV | Derived from player_game_stats | **ZERO** - Exact match |

**Key Achievement**: All MoneyPuck columns preserved including:
- ‚úÖ `situation` column (all, 5on4, 4on5, 5on5)
- ‚úÖ OnIce/OffIce stats (17 columns)
- ‚úÖ Shot danger metrics (lowDanger, mediumDanger, highDanger)
- ‚úÖ Corsi/Fenwick percentages
- ‚úÖ Rebounds, saved shots, zone continuation

---

## Pre-Deployment Checklist

### 1. Local Testing (COMPLETED ‚úÖ)
- [x] nhl-api-py installed and working
- [x] Test suite runs successfully (4/6 tests PASS)
- [x] Parser functions validated with real NHL data
- [x] Known issues documented

### 2. Databricks Environment Setup (TODO)
- [ ] Create Databricks cluster with:
  - Runtime: 14.3 LTS or later
  - Python 3.10+
  - PyPI library: `nhl-api-py==3.1.1`
- [ ] Upload source files to Databricks workspace
- [ ] Configure DLT pipeline parameters

### 3. Configuration Parameters
The new pipeline requires these Spark configurations:

```python
# Required parameters
spark.conf.set("catalog", "lr_nhl_demo")  # Your catalog name
spark.conf.set("schema", "dev")  # Your schema name
spark.conf.set("bundle.sourcePath", "/Workspace/path/to/nhlPredict/src")

# Optional parameters
spark.conf.set("one_time_load", "false")  # "true" for full historical, "false" for incremental
spark.conf.set("volume_path", f"/Volumes/{catalog}/{schema}/nhl_raw_data")  # Storage location
```

---

## Deployment Steps

### Step 1: Upload Files to Databricks Workspace

```bash
# Using Databricks CLI
databricks workspace import-dir \
  nhlPredict/src \
  /Workspace/Repos/<username>/NHL_SOG/nhlPredict/src \
  --overwrite
```

Or manually upload via Databricks UI:
1. Navigate to Workspace ‚Üí Repos
2. Upload `/src/utils/nhl_api_helper.py`
3. Upload `/src/dlt_etl/ingestion/01-bronze-ingestion-nhl-api.py`

### Step 2: Create DLT Pipeline

1. **In Databricks UI**:
   - Navigate to **Delta Live Tables** ‚Üí **Create Pipeline**
   - Name: `NHL Bronze Layer - NHL API (Test)`
   - Notebook: `/Workspace/.../01-bronze-ingestion-nhl-api.py`

2. **Configure Pipeline Settings**:
   ```json
   {
     "catalog": "lr_nhl_demo",
     "schema": "dev",
     "one_time_load": "false"
   }
   ```

3. **Cluster Configuration**:
   - Mode: **Enhanced Autoscaling**
   - Min workers: 1
   - Max workers: 4
   - Libraries: `nhl-api-py==3.1.1` (PyPI)

4. **Storage**:
   - Target: `lr_nhl_demo.dev`
   - Storage location: `/pipelines/nhl_bronze_nhl_api`

### Step 3: Initial Test Run (Single Date)

**Recommended**: Start with a small test before full load.

**Edit line 58-59 in `01-bronze-ingestion-nhl-api.py`**:
```python
# TEST MODE: Single date
if one_time_load:
    start_date = datetime(2024, 10, 8).date()  # Single day for testing
    end_date = datetime(2024, 10, 8).date()
    print(f"üìÖ TEST LOAD: {start_date}")
else:
    # Incremental load (yesterday only)
    start_date = today - timedelta(days=1)
    end_date = today
```

**Run the pipeline**:
1. Click **Start** in DLT UI
2. Monitor execution in **Event Log**
3. Expected runtime: ~5-10 minutes for single date

**Expected Output**:
```
üèí Starting NHL API ingestion: 2024-10-08 to 2024-10-08
üìÖ Processing 1 dates
üìÖ Processing date: 2024-10-08
  üéÆ Found 6 games on 2024-10-08
    Processing game 2024020001: STL @ SEA
      ‚úÖ Game 2024020001 processed successfully
    Processing game 2024020002: VAN @ PHI
      ‚úÖ Game 2024020002 processed successfully
    ...
‚úÖ Ingestion complete: 6 games processed, 240 player-game records
```

### Step 4: Validate Output

**Query the bronze tables**:

```sql
-- Check row counts
SELECT COUNT(*) FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api;
-- Expected: ~240 rows (40 players √ó 6 games)

SELECT COUNT(*) FROM lr_nhl_demo.dev.bronze_games_historical_v2_nhl_api;
-- Expected: 48 rows (2 teams √ó 6 games √ó 4 situations)

-- Sample data
SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api
WHERE gameId = 2024020001
LIMIT 10;

-- Verify situation column
SELECT situation, COUNT(*) as count
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api
GROUP BY situation;
-- Expected: all, 5on4, 4on5, 5on5

-- Verify stats match known game results
SELECT 
  playerTeam,
  situation,
  SUM(I_F_goals) as total_goals,
  SUM(I_F_shotsOnGoal) as total_shots
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api
WHERE gameId = 2024020001 AND situation = 'all'
GROUP BY playerTeam, situation;
-- Compare against NHL.com box score for game 2024020001
```

### Step 5: Compare Against MoneyPuck Data

**Validation queries**:

```sql
-- Compare schemas (should be identical)
DESCRIBE lr_nhl_demo.dev.bronze_player_game_stats_v2;  -- Old
DESCRIBE lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api;  -- New

-- Compare sample game data
SELECT 
  playerId,
  name,
  playerTeam,
  situation,
  I_F_shotsOnGoal,
  I_F_goals,
  OnIce_F_shotsOnGoal,
  corsiFor
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2
WHERE gameId = 2024020001 AND situation = 'all'
ORDER BY I_F_shotsOnGoal DESC
LIMIT 10;

-- Run same query on new table and compare results
SELECT 
  playerId,
  name,
  playerTeam,
  situation,
  I_F_shotsOnGoal,
  I_F_goals,
  OnIce_F_shotsOnGoal,
  corsiFor
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_nhl_api
WHERE gameId = 2024020001 AND situation = 'all'
ORDER BY I_F_shotsOnGoal DESC
LIMIT 10;
```

**Expected differences**:
- **xGoalsFor/xGoalsAgainst**: Will be NULL in new data (MoneyPuck proprietary model)
- **Minor stat variations**: ¬±5% acceptable due to parsing methodology differences

**Acceptable variance**:
- Shots, goals: Should match exactly
- Corsi/Fenwick: ¬±2-3% acceptable
- OnIce/OffIce stats: ¬±5% acceptable (shift timing precision)

### Step 6: Full Historical Load (Optional)

Once single-date test passes, run full historical load:

1. **Update configuration**:
   ```python
   # In 01-bronze-ingestion-nhl-api.py, lines 58-59
   if one_time_load:
       start_date = datetime(2023, 10, 1).date()  # Full 2023-24 season
       end_date = today
   ```

2. **Set pipeline parameter**:
   ```json
   {
     "one_time_load": "true"
   }
   ```

3. **Run pipeline**:
   - Expected runtime: ~4-6 hours for full season
   - Monitor for API rate limiting errors
   - Check logs for any game processing failures

4. **Incremental loads**:
   - After full load, set `one_time_load: "false"`
   - Pipeline will fetch yesterday's games only
   - Schedule daily runs via Databricks Jobs

---

## Validation Checklist

### Bronze Layer Validation

- [ ] All four bronze tables created successfully
- [ ] Row counts reasonable (players √ó games √ó situations)
- [ ] `situation` column has correct values: all, 5on4, 4on5, 5on5
- [ ] Stats match known game results (check NHL.com box scores)
- [ ] No NULL values in required columns (playerId, playerTeam, season, situation)
- [ ] OnIce/OffIce columns populated (not all zeros)
- [ ] Shot danger columns populated (lowDanger, mediumDanger, highDanger)
- [ ] Corsi/Fenwick percentages reasonable (40-60% range)

### Silver Layer Validation (Next Step)

```sql
-- Test if existing silver transformations work unchanged
CREATE OR REFRESH LIVE TABLE silver_schedule_2023_v2_test AS
SELECT * FROM lr_nhl_demo.dev.bronze_schedule_2023_v2_nhl_api;

CREATE OR REFRESH LIVE TABLE silver_games_schedule_v2_test AS
SELECT 
  gameId,
  gameDate,
  team,
  opposingTeam,
  -- ... (copy existing silver transformations)
FROM lr_nhl_demo.dev.bronze_games_historical_v2_nhl_api;
```

**Expected result**: Silver queries run without errors (ZERO code changes needed)

---

## Troubleshooting

### Common Issues

#### 1. "ModuleNotFoundError: No module named 'nhlpy'"

**Solution**: Install nhl-api-py on cluster
```bash
# Via UI: Cluster ‚Üí Libraries ‚Üí Install New ‚Üí PyPI ‚Üí "nhl-api-py==3.1.1"
# Or via CLI:
databricks libraries install --cluster-id <cluster-id> --pypi-package nhl-api-py==3.1.1
```

#### 2. "ImportError: cannot import name 'fetch_with_retry'"

**Solution**: Ensure `bundle.sourcePath` is set correctly
```python
spark.conf.set("bundle.sourcePath", "/Workspace/path/to/nhlPredict/src")
```

#### 3. API Rate Limiting (429 errors)

**Solution**: Increase retry delays
```python
# In nhl_api_helper.py, line 11
time.sleep(2)  # Increase from 1 to 2 seconds
```

#### 4. Zero stats returned (all zeros)

**Solution**: Check team ID mapping
- Verify boxscore team IDs match play-by-play eventOwnerTeamId
- See `KNOWN_ISSUES.md` section on Team ID Mapping

#### 5. Shift data missing

**Solution**: Some games may not have shift data
- Check API response structure: `shifts['data']` vs `shifts`
- Gracefully handle missing shift data (OnIce/OffIce stats will be zeros)

---

## Rollback Plan

If issues are found during deployment:

### Option 1: Quick Rollback
1. Switch DLT pipeline back to `01-bronze-ingestion.py` (old MoneyPuck version)
2. Downstream pipelines continue working unchanged
3. No data loss

### Option 2: Parallel Operation
1. Run both pipelines simultaneously:
   - Old: `bronze_player_game_stats_v2` (MoneyPuck)
   - New: `bronze_player_game_stats_v2_nhl_api` (NHL API)
2. Silver/gold layers point to old tables
3. Validate new tables in parallel
4. Switch over when confident

---

## Success Criteria

Before marking deployment complete:

‚úÖ **Bronze Layer**:
- [ ] All 4 bronze tables created
- [ ] Row counts match expected (players √ó games √ó situations)
- [ ] Stats validated against NHL.com box scores
- [ ] Known issues acceptable (documented in `KNOWN_ISSUES.md`)

‚úÖ **Silver Layer**:
- [ ] Existing silver transformations run unchanged
- [ ] Output validated against old silver tables
- [ ] No code changes required

‚úÖ **Gold Layer**:
- [ ] Existing gold aggregations run unchanged
- [ ] Output validated against old gold tables
- [ ] No code changes required

‚úÖ **ML Pipeline**:
- [ ] Feature engineering runs unchanged
- [ ] Model predictions comparable to baseline
- [ ] No retraining required

‚úÖ **BI/Dashboard**:
- [ ] Dashboard queries run unchanged
- [ ] Visualizations display correctly
- [ ] LLM generation works

---

## Next Steps After Successful Deployment

1. **Monitor Performance**:
   - Track API response times
   - Monitor DLT pipeline execution time
   - Check for any data quality issues

2. **Optimize**:
   - Implement parallelization for faster processing
   - Cache raw API responses before parsing
   - Tune retry logic based on observed error rates

3. **Enhance**:
   - Add expected goals (xGoals) model
   - Fix situation classifier 5-on-3 edge case
   - Tune shot danger thresholds

4. **Document**:
   - Update README with new data source
   - Document API usage patterns
   - Create operational runbook

---

## Contact & Support

**Documentation**:
- Main plan: `nhl_api_migration_plan_dd31acdc.plan.md`
- Known issues: `KNOWN_ISSUES.md`
- Migration status: `MIGRATION_STATUS.md`

**Code**:
- Bronze layer: `/src/dlt_etl/ingestion/01-bronze-ingestion-nhl-api.py`
- Parser: `/src/utils/nhl_api_helper.py`
- Tests: `/src/utils/test_nhl_api_validation.py`

**External Resources**:
- nhl-api-py docs: https://pypi.org/project/nhl-api-py/
- NHL API: https://api-web.nhle.com/v1/
- Databricks DLT: https://docs.databricks.com/aws/en/ldp/

---

**Document Version**: 1.0  
**Last Updated**: 2026-01-30  
**Status**: Ready for Databricks deployment
