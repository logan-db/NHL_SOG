# Migration Guide: Staging Pattern ‚Üí Simplified Architecture

**Date:** 2026-02-04  
**Objective:** Migrate from complex staging pattern to simplified `@dlt.table()` approach  
**Expected Downtime:** 10-15 minutes (backup load + first incremental run)

---

## üéØ What We're Doing

**From:** Complex staging pattern (2x tables per source, streaming tables, skipChangeCommits)  
**To:** Simple `@dlt.table()` with smart incremental logic (Databricks recommended)

**Why:**
- **Simpler:** 50% less code (~600 lines removed)
- **Faster:** 5-10 min rebuilds (vs 4-5 hours)
- **Proven:** 90% of production DLT pipelines use this pattern
- **Adequate protection:** `pipelines.reset.allowed: "false"` + backups + time travel
- **CRITICAL FIX:** Adds future schedule fetching for ML predictions!

---

## üìã Pre-Migration Checklist

### 1. Verify Backups Exist

```sql
-- Check bronze table backups exist
SELECT 
    'bronze_player_game_stats_v2_backup' as table_name,
    COUNT(*) as records,
    MIN(gameDate) as min_date,
    MAX(gameDate) as max_date
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_backup

UNION ALL

SELECT 
    'bronze_games_historical_v2_backup',
    COUNT(*),
    MIN(gameDate),
    MAX(gameDate)
FROM lr_nhl_demo.dev.bronze_games_historical_v2_backup;
```

**Expected:**
- `bronze_player_game_stats_v2_backup`: ~492K records
- `bronze_games_historical_v2_backup`: ~31K records

### 2. Verify Silver/Gold Tables Work

```sql
-- Quick validation
SELECT COUNT(*) FROM lr_nhl_demo.dev.silver_player_stats_v2;
SELECT COUNT(*) FROM lr_nhl_demo.dev.gold_model_stats_v2;
```

**Expected:**
- Silver: ~123K records
- Gold: ~123K records

---

## üöÄ Migration Steps

### Step 1: Deploy New Code (NO DROPS YET)

```bash
cd /Users/logan.rupert/Library/CloudStorage/GoogleDrive-logan.rupert@databricks.com/My\ Drive/Repositories/NHL_SOG/nhlPredict

# Deploy with new simplified code
databricks bundle deploy --target dev
```

**What this does:**
- Uploads new simplified ingestion notebook
- Updates pipeline configuration
- Does NOT drop existing tables

### Step 2: Drop Old Tables (Bronze Only)

```sql
-- Drop bronze tables (staging pattern tables)
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_schedule_2023_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_skaters_2023_v2;

-- Drop staging tables if they exist
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2_staging;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2_staging;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2_staging_manual;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2_staging_manual;

-- KEEP silver and gold tables (no changes needed)
```

### Step 3: Load Backups into Bronze Tables

```sql
-- Restore bronze_player_game_stats_v2 from backup
CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2
AS SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_backup;

-- Add table properties manually (DLT will adopt on first run)
ALTER TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2 
SET TBLPROPERTIES (
  'pipelines.reset.allowed' = 'false',
  'quality' = 'bronze',
  'source' = 'nhl-api-py'
);

-- Restore bronze_games_historical_v2 from backup
CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_games_historical_v2
AS SELECT * FROM lr_nhl_demo.dev.bronze_games_historical_v2_backup;

ALTER TABLE lr_nhl_demo.dev.bronze_games_historical_v2 
SET TBLPROPERTIES (
  'pipelines.reset.allowed' = 'false',
  'quality' = 'bronze',
  'source' = 'nhl-api-py'
);

-- Validate
SELECT 
    'bronze_player_game_stats_v2' as table_name,
    COUNT(*) as records,
    MIN(gameDate) as min_date,
    MAX(gameDate) as max_date
FROM lr_nhl_demo.dev.bronze_player_game_stats_v2

UNION ALL

SELECT 
    'bronze_games_historical_v2',
    COUNT(*),
    MIN(gameDate),
    MAX(gameDate)
FROM lr_nhl_demo.dev.bronze_games_historical_v2;
```

**Expected Output:**
```
table_name                        | records | min_date | max_date
----------------------------------|---------|----------|----------
bronze_player_game_stats_v2       | 492572  | 20231010 | 20250129
bronze_games_historical_v2        | 31640   | 20231010 | 20250129
```

### Step 4: Update Pipeline Configuration

**File:** `nhlPredict/resources/NHLPlayerIngestion.yml`

```yaml
resources:
  pipelines:
    NHLPlayerIngestion:
      name: NHLPlayerIngestion
      target: ${bundle.environment}
      continuous: false
      channel: PREVIEW
      photon: true
      libraries:
        - notebook:
            path: ../src/dlt_etl/ingestion/01-bronze-ingestion-nhl-api-SIMPLIFIED.py  # ‚Üê NEW FILE
        - notebook:
            path: ../src/dlt_etl/transformation/02-silver-transform.py
        - notebook:
            path: ../src/dlt_etl/aggregation/03-gold-agg.py
      serverless: true
      catalog: lr_nhl_demo
      configuration:
        bundle.sourcePath: ${workspace.file_path}/src
        one_time_load: "false"     # ‚Üê INCREMENTAL MODE
        lookback_days: "1"         # ‚Üê Safety buffer
      notifications:
        - email_recipients:
            - logan.rupert@databricks.com
          alerts:
            - on-update-success
            - on-update-failure
            - on-flow-failure
      root_path: /Workspace/Users/logan.rupert@databricks.com/.bundle/nhlPredict/dev/files/src/dlt_etl
```

**Changes:**
- ‚úÖ Changed notebook path to `01-bronze-ingestion-nhl-api-SIMPLIFIED.py`
- ‚úÖ Removed `skip_staging_ingestion` parameter
- ‚úÖ Kept `one_time_load: "false"` for incremental mode

### Step 5: Deploy Updated Configuration

```bash
cd /Users/logan.rupert/Library/CloudStorage/GoogleDrive-logan.rupert@databricks.com/My\ Drive/Repositories/NHL_SOG/nhlPredict

# Deploy updated config
databricks bundle deploy --target dev
```

### Step 6: Run Pipeline (First Incremental Run)

**Option A: Via Databricks CLI**
```bash
databricks pipelines start <pipeline-id> --full-refresh false
```

**Option B: Via UI**
1. Go to Databricks Workspace ‚Üí Workflows ‚Üí Delta Live Tables
2. Find `NHLPlayerIngestion` pipeline
3. Click "Start"
4. **Do NOT select "Full Refresh"** - just click Start

**Expected Behavior:**
- ‚úÖ Bronze tables: Append only new data from yesterday + today (5-10 min)
- ‚úÖ Schedule table: Historical games + **FUTURE GAMES** for predictions!
- ‚úÖ Silver/gold: Rebuilt from bronze (no code changes, same logic)
- ‚úÖ Runtime: 10-15 minutes total

### Step 7: Validate ML Readiness

```sql
-- Quick validation query
WITH upcoming_games AS (
  SELECT 
    gameDate,
    playerId,
    shooterName,
    position,
    playerGamesPlayedRolling,
    player_Total_shotsOnGoal
  FROM lr_nhl_demo.dev.gold_model_stats_v2
  WHERE gameDate >= CAST(DATE_FORMAT(CURRENT_DATE(), 'yyyyMMdd') AS INT)
  LIMIT 10
)
SELECT 
  'üéØ UPCOMING GAMES' as info,
  COUNT(*) as total_upcoming_records
FROM upcoming_games

UNION ALL

SELECT 
  'üèí HISTORICAL GAMES',
  COUNT(*)
FROM lr_nhl_demo.dev.gold_model_stats_v2
WHERE gameDate < CAST(DATE_FORMAT(CURRENT_DATE(), 'yyyyMMdd') AS INT);
```

**Expected:**
- ‚úÖ Upcoming games: **300-500 records** (THIS IS THE KEY FIX!)
- ‚úÖ Historical games: ~123K records

### Step 8: Test Next-Day Incremental

**Tomorrow (2026-02-05):**

```bash
# Trigger pipeline manually or wait for schedule
databricks pipelines start <pipeline-id> --full-refresh false
```

**Expected:**
- ‚úÖ Runtime: 5-10 minutes (only processes yesterday + today)
- ‚úÖ No duplicates (smart date logic + deduplication)
- ‚úÖ Bronze appends new data (does NOT drop/recreate)

---

## ‚úÖ Success Criteria

| Metric | Target | How to Verify |
|--------|--------|---------------|
| **Bronze records** | ~492K+ | `SELECT COUNT(*) FROM bronze_player_game_stats_v2` |
| **Upcoming games** | 300-500 | Query gold table for `gameDate >= today` |
| **Incremental runtime** | 5-10 min | Check pipeline execution time |
| **Rebuild runtime** | 5-10 min | Deploy code change, check runtime |
| **No duplicates** | 0 | Check `bronze_player_game_stats_v2` for duplicate `(playerId, gameId, situation)` |
| **Silver works** | 123K+ | `SELECT COUNT(*) FROM silver_player_stats_v2` |
| **Gold works** | 123K+ | `SELECT COUNT(*) FROM gold_model_stats_v2` |

---

## üõ°Ô∏è Data Protection Strategy

### Layer 1: Configuration
```yaml
table_properties:
  pipelines.reset.allowed: "false"  # Prevents manual resets
```

### Layer 2: Smart Incremental Logic
- Even on code changes, table rebuilds in 5-10 min (only fetches recent dates)
- No 4-5 hour API reload needed!

### Layer 3: Regular Backups
```sql
-- Weekly backup job (to be automated)
CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2_backup
AS SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2;

CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_games_historical_v2_backup
AS SELECT * FROM lr_nhl_demo.dev.bronze_games_historical_v2;
```

### Layer 4: Delta Time Travel
```sql
-- Recover from any point in last 30 days
SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2 
VERSION AS OF 5;  -- 5 versions ago

SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2 
TIMESTAMP AS OF '2026-02-01';  -- Specific date
```

---

## üßπ Post-Migration Cleanup (Optional)

After validating the migration for 1 week:

```sql
-- Drop old staging manual tables
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2_staging_manual;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2_staging_manual;

-- Archive old ingestion notebook
-- Rename: 01-bronze-ingestion-nhl-api.py ‚Üí 01-bronze-ingestion-nhl-api-OLD-STAGING-PATTERN.py
```

---

## üÜò Rollback Plan

If something goes wrong:

### Option 1: Restore from Backup (Fast - 2 minutes)

```sql
-- Restore bronze tables
CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2
AS SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_backup;

CREATE OR REPLACE TABLE lr_nhl_demo.dev.bronze_games_historical_v2
AS SELECT * FROM lr_nhl_demo.dev.bronze_games_historical_v2_backup;
```

### Option 2: Revert to Old Code

```bash
# Revert notebook path in NHLPlayerIngestion.yml
# Change back to: ../src/dlt_etl/ingestion/01-bronze-ingestion-nhl-api.py

databricks bundle deploy --target dev
```

### Option 3: Delta Time Travel (Last Resort)

```sql
RESTORE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2 
TO VERSION AS OF 10;
```

---

## üìä Comparison: Old vs New

| Aspect | Staging Pattern (Old) | Simplified (New) |
|--------|----------------------|-----------------|
| **Code complexity** | ~1,200 lines | ~600 lines |
| **Tables per source** | 2 (staging + final) | 1 |
| **Rebuild time** | 4-5 hours | 5-10 min |
| **Incremental time** | 5-10 min | 5-10 min |
| **Data protection** | streaming tables | config + backups |
| **Debugging** | Complex (checkpoints) | Simple |
| **Storage cost** | +20% (2x tables) | Baseline |
| **Maintenance** | High | Low |
| **Databricks recommended** | No | Yes ‚úÖ |

---

## üîç Key Differences in New Code

### 1. Removed Staging Pattern Complexity

**OLD (Staging Pattern):**
```python
# Staging table (batch)
@dlt.table(name="bronze_player_game_stats_v2_staging")
def staging():
    return fetch_from_api()

# Final table (streaming from staging)
dlt.create_streaming_table(name="bronze_player_game_stats_v2")

@dlt.append_flow(target="bronze_player_game_stats_v2")
def stream_from_staging():
    return spark.readStream.table("...staging").option("skipChangeCommits", "true")
```

**NEW (Simplified):**
```python
# Single table, simple pattern
@dlt.table(
    name="bronze_player_game_stats_v2",
    table_properties={"pipelines.reset.allowed": "false"}
)
def ingest():
    return fetch_from_api()
```

### 2. Added Future Schedule Fetching (CRITICAL!)

**OLD:**
```python
# Only derived schedule from historical games
def ingest_schedule_v2():
    games_df = dlt.read("bronze_games_historical_v2")
    return derive_schedule(games_df)  # ‚ùå No future games!
```

**NEW:**
```python
# Historical + Future schedule
def ingest_schedule_v2():
    # Part 1: Historical from games
    historical = derive_schedule(games_df)
    
    # Part 2: Future from API (CRITICAL!)
    future = fetch_future_schedule_from_api(next_7_days)
    
    # Part 3: Combine
    return historical.union(future).dropDuplicates(["GAME_ID"])
```

---

## üìö Additional Resources

- **Best Practice Doc:** `nhlPredict/cursor_docs/BEST_PRACTICE_ARCHITECTURE_FINAL.md`
- **DLT Docs:** https://docs.databricks.com/delta-live-tables/
- **Transcript:** `/Users/logan.rupert/.cursor/projects/.../agent-transcripts/5e48e730-b540-4c8b-ab51-2b275d6ed051.txt`

---

## ‚úÖ Next Steps

1. **Review this migration guide**
2. **Verify backups exist** (Step 1)
3. **Execute migration steps** (Steps 1-7)
4. **Validate results** (Step 7-8)
5. **Monitor next-day incremental** (Step 8)
6. **Clean up after 1 week** (Post-migration cleanup)

---

**Ready to migrate?** Just say "Let's migrate" and I'll execute these steps! üöÄ
