# Databricks notebook source
# MAGIC %pip install nhl-api-py==3.1.1

# COMMAND ----------

# MAGIC %md
# MAGIC ## NHL API Bronze Layer Pipeline
# MAGIC
# MAGIC This pipeline ingests data from the official NHL API using nhl-api-py
# MAGIC and transforms it into MoneyPuck-compatible schema.
# MAGIC
# MAGIC **Data Source**: NHL API via nhl-api-py library
# MAGIC **Target Schema**: MoneyPuck-compatible (maintains backward compatibility)
# MAGIC **Strategy**: Zero downstream changes - preserves all existing columns

# COMMAND ----------

# MAGIC %md
# MAGIC ### Imports and Configuration

# COMMAND ----------

# DBTITLE 1,Imports
import dlt
import sys
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime, timedelta, date
from typing import List, Dict, Any
import time

# Add project to path
sys.path.append(spark.conf.get("bundle.sourcePath", "."))

# Import NHL API helper functions
from utils.nhl_api_helper import (
    fetch_with_retry,
    aggregate_team_stats_by_situation,
    aggregate_player_stats_by_situation,
    classify_situation,
    classify_shot_danger,
    is_player_on_ice,
)

# COMMAND ----------

# DBTITLE 1,NHL API Client Setup
from nhlpy import NHLClient

# Initialize NHL API client (no authentication required)
nhl_client = NHLClient()

print("‚úÖ NHL API client initialized successfully")

# COMMAND ----------

# DBTITLE 1,Configuration Parameters
# Get configuration from Databricks job/pipeline parameters
catalog = spark.conf.get("catalog", "lr_nhl_demo")
schema = spark.conf.get("schema", "dev")
volume_path = spark.conf.get("volume_path", f"/Volumes/{catalog}/{schema}/nhl_raw_data")

# Date range for data ingestion
one_time_load = spark.conf.get("one_time_load", "false").lower() == "true"
skip_staging_ingestion = (
    spark.conf.get("skip_staging_ingestion", "false").lower() == "true"
)  # Use existing staging data (skip API calls)
lookback_days = int(
    spark.conf.get("lookback_days", "1")
)  # Safety buffer for late-arriving data (default: 1 day)

# Note: NHL API returns season in 8-digit format (e.g., 20232024 for 2023-24 season)
season_list = [20232024, 20242025, 20252026]  # Current and recent seasons

# Get date range
today = date.today()

if one_time_load:
    # HISTORICAL LOAD: Full season(s) of data
    # Load last 2+ seasons (2023-24, 2024-25, 2025-26)
    start_date = datetime(2023, 10, 1).date()  # Start of 2023-24 season
    end_date = today
    print(f"üìÖ HISTORICAL LOAD: {start_date} to {end_date}")
    print(f"   Date range: {(end_date - start_date).days} days")
    print(f"   Estimated games: ~{(end_date - start_date).days * 8}")
    print(f"   Estimated runtime: 3-5 hours")
else:
    # INCREMENTAL LOAD: Only new data (no duplicates)
    # Skip incremental query in skip mode (streaming tables may not exist yet)
    if skip_staging_ingestion:
        # Skip mode: Just set a reasonable date range (won't be used anyway)
        start_date = today - timedelta(days=lookback_days)
        end_date = today
        print(f"üìÖ SKIP MODE: Date range not used (loading from manual staging tables)")
    else:
        # Normal incremental: Check max date from existing bronze table
        try:
            max_date_result = spark.sql(
                f"""
                SELECT MAX(gameDate) as max_date 
                FROM {catalog}.{schema}.bronze_player_game_stats_v2
            """
            ).collect()

            if max_date_result and max_date_result[0]["max_date"] is not None:
                # Convert gameDate (int format YYYYMMDD) to date object
                max_date_str = str(max_date_result[0]["max_date"])
                max_date = datetime.strptime(max_date_str, "%Y%m%d").date()

                # Start from day after max date (minus lookback_days as safety buffer)
                start_date = max_date - timedelta(days=lookback_days)
                end_date = today

                print(f"üìÖ INCREMENTAL LOAD (Smart Mode):")
                print(f"   Last processed date: {max_date}")
                print(
                    f"   Processing from: {start_date} (includes {lookback_days}-day safety buffer)"
                )
                print(f"   Processing to: {end_date}")
                print(f"   New date range: {(end_date - start_date).days} days")
                print(f"   Estimated new games: ~{(end_date - start_date).days * 8}")
                print(f"   ‚úÖ No duplicates - only processing new/recent data")
            else:
                # Table exists but is empty - treat as first load
                print(f"‚ö†Ô∏è  Bronze table exists but is empty - performing initial load")
                start_date = datetime(2023, 10, 1).date()
                end_date = today
                print(f"üìÖ INITIAL LOAD: {start_date} to {end_date}")

        except Exception as e:
            # Table doesn't exist yet - use fallback to lookback_days
            print(f"‚ö†Ô∏è  Bronze table doesn't exist yet (expected on first run)")
            start_date = today - timedelta(days=lookback_days)
            end_date = today
            print(f"üìÖ INCREMENTAL LOAD (Fallback Mode): {start_date} to {end_date}")
            print(f"   Will process last {lookback_days} days")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Helper Functions

# COMMAND ----------


# DBTITLE 1,Date Range Generator
def generate_date_range(start: date, end: date) -> List[str]:
    """Generate list of date strings in YYYY-MM-DD format."""
    dates = []
    current = start
    while current <= end:
        dates.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    return dates


def extract_player_name(player: Dict) -> str:
    """
    Extract player name from NHL API player object.

    Handles both nested dict format (with 'default' key) and simple string format.

    Args:
        player: Player dict from NHL API boxscore

    Returns:
        str: Full player name (e.g., "Connor McDavid")
    """
    first_name = player.get("firstName", {})
    last_name = player.get("lastName", {})

    # If firstName/lastName are dicts (with 'default' key), extract the value
    if isinstance(first_name, dict):
        first_name = first_name.get("default", "")
    elif not isinstance(first_name, str):
        first_name = ""

    if isinstance(last_name, dict):
        last_name = last_name.get("default", "")
    elif not isinstance(last_name, str):
        last_name = ""

    return f"{first_name} {last_name}".strip()


# COMMAND ----------


# DBTITLE 1,Schema Definitions
def get_player_game_stats_schema():
    """
    Returns the schema for bronze_player_game_stats_v2 table.
    Matches MoneyPuck schema exactly to ensure zero downstream changes.
    """
    return StructType(
        [
            # Identifiers
            StructField("playerId", StringType(), False),
            StructField("playerTeam", StringType(), False),
            StructField("opposingTeam", StringType(), True),
            StructField("name", StringType(), True),
            StructField("position", StringType(), True),
            StructField("home_or_away", StringType(), True),  # "HOME" or "AWAY"
            # Game Info
            StructField("gameId", IntegerType(), True),
            StructField("gameDate", IntegerType(), True),  # YYYYMMDD format
            StructField("season", IntegerType(), False),
            StructField(
                "situation", StringType(), False
            ),  # "all", "5on4", "4on5", "5on5"
            # Time on Ice
            StructField("icetime", DoubleType(), True),
            StructField("iceTimeRank", IntegerType(), True),
            StructField("shifts", IntegerType(), True),
            # Shooting Stats
            StructField("I_F_shotsOnGoal", IntegerType(), True),
            StructField("I_F_missedShots", IntegerType(), True),
            StructField("I_F_blockedShotAttempts", IntegerType(), True),
            StructField("I_F_shotAttempts", IntegerType(), True),
            StructField("I_F_unblockedShotAttempts", IntegerType(), True),
            StructField("I_F_goals", IntegerType(), True),
            StructField("I_F_primaryAssists", IntegerType(), True),
            StructField("I_F_secondaryAssists", IntegerType(), True),
            StructField("I_F_points", IntegerType(), True),
            StructField("I_F_rebounds", IntegerType(), True),
            StructField("I_F_reboundGoals", IntegerType(), True),
            StructField("I_F_savedShotsOnGoal", IntegerType(), True),
            StructField("I_F_savedUnblockedShotAttempts", IntegerType(), True),
            # Individual Events
            StructField("I_F_hits", IntegerType(), True),
            StructField("I_F_takeaways", IntegerType(), True),
            StructField("I_F_giveaways", IntegerType(), True),
            # Individual Shot Danger (I_F_* prefix for player's own shots)
            StructField("I_F_lowDangerShots", IntegerType(), True),
            StructField("I_F_mediumDangerShots", IntegerType(), True),
            StructField("I_F_highDangerShots", IntegerType(), True),
            StructField("I_F_lowDangerGoals", IntegerType(), True),
            StructField("I_F_mediumDangerGoals", IntegerType(), True),
            StructField("I_F_highDangerGoals", IntegerType(), True),
            # Shot Danger (For)
            StructField("shotsOnGoalFor", IntegerType(), True),
            StructField("missedShotsFor", IntegerType(), True),
            StructField("blockedShotAttemptsFor", IntegerType(), True),
            StructField("shotAttemptsFor", IntegerType(), True),
            StructField("unblockedShotAttemptsFor", IntegerType(), True),
            StructField("goalsFor", IntegerType(), True),
            StructField("reboundsFor", IntegerType(), True),
            StructField("reboundGoalsFor", IntegerType(), True),
            StructField("savedShotsOnGoalFor", IntegerType(), True),
            StructField("savedUnblockedShotAttemptsFor", IntegerType(), True),
            StructField("lowDangerShotsFor", IntegerType(), True),
            StructField("mediumDangerShotsFor", IntegerType(), True),
            StructField("highDangerShotsFor", IntegerType(), True),
            StructField("lowDangerGoalsFor", IntegerType(), True),
            StructField("mediumDangerGoalsFor", IntegerType(), True),
            StructField("highDangerGoalsFor", IntegerType(), True),
            # Shot Danger (Against)
            StructField("shotsOnGoalAgainst", IntegerType(), True),
            StructField("missedShotsAgainst", IntegerType(), True),
            StructField("blockedShotAttemptsAgainst", IntegerType(), True),
            StructField("shotAttemptsAgainst", IntegerType(), True),
            StructField("unblockedShotAttemptsAgainst", IntegerType(), True),
            StructField("goalsAgainst", IntegerType(), True),
            StructField("reboundsAgainst", IntegerType(), True),
            StructField("reboundGoalsAgainst", IntegerType(), True),
            StructField("savedShotsOnGoalAgainst", IntegerType(), True),
            StructField("savedUnblockedShotAttemptsAgainst", IntegerType(), True),
            StructField("lowDangerShotsAgainst", IntegerType(), True),
            StructField("mediumDangerShotsAgainst", IntegerType(), True),
            StructField("highDangerShotsAgainst", IntegerType(), True),
            StructField("lowDangerGoalsAgainst", IntegerType(), True),
            StructField("mediumDangerGoalsAgainst", IntegerType(), True),
            StructField("highDangerGoalsAgainst", IntegerType(), True),
            # OnIce Stats (Team performance while player is on ice)
            StructField("OnIce_F_shotsOnGoal", IntegerType(), True),
            StructField("OnIce_F_missedShots", IntegerType(), True),
            StructField("OnIce_F_blockedShotAttempts", IntegerType(), True),
            StructField("OnIce_F_shotAttempts", IntegerType(), True),
            StructField("OnIce_F_unblockedShotAttempts", IntegerType(), True),
            StructField("OnIce_F_goals", IntegerType(), True),
            StructField("OnIce_F_lowDangerShots", IntegerType(), True),
            StructField("OnIce_F_mediumDangerShots", IntegerType(), True),
            StructField("OnIce_F_highDangerShots", IntegerType(), True),
            StructField("OnIce_F_lowDangerGoals", IntegerType(), True),
            StructField("OnIce_F_mediumDangerGoals", IntegerType(), True),
            StructField("OnIce_F_highDangerGoals", IntegerType(), True),
            StructField("OnIce_A_shotsOnGoal", IntegerType(), True),
            StructField("OnIce_A_missedShots", IntegerType(), True),
            StructField("OnIce_A_blockedShotAttempts", IntegerType(), True),
            StructField("OnIce_A_shotAttempts", IntegerType(), True),
            StructField("OnIce_A_unblockedShotAttempts", IntegerType(), True),
            StructField("OnIce_A_goals", IntegerType(), True),
            StructField("OnIce_A_lowDangerShots", IntegerType(), True),
            StructField("OnIce_A_mediumDangerShots", IntegerType(), True),
            StructField("OnIce_A_highDangerShots", IntegerType(), True),
            StructField("OnIce_A_lowDangerGoals", IntegerType(), True),
            StructField("OnIce_A_mediumDangerGoals", IntegerType(), True),
            StructField("OnIce_A_highDangerGoals", IntegerType(), True),
            # OffIce Stats (Team performance while player is off ice)
            StructField("OffIce_F_shotsOnGoal", IntegerType(), True),
            StructField("OffIce_F_missedShots", IntegerType(), True),
            StructField("OffIce_F_blockedShotAttempts", IntegerType(), True),
            StructField("OffIce_F_shotAttempts", IntegerType(), True),
            StructField("OffIce_F_unblockedShotAttempts", IntegerType(), True),
            StructField("OffIce_F_goals", IntegerType(), True),
            StructField("OffIce_A_shotsOnGoal", IntegerType(), True),
            StructField("OffIce_A_missedShots", IntegerType(), True),
            StructField("OffIce_A_blockedShotAttempts", IntegerType(), True),
            StructField("OffIce_A_shotAttempts", IntegerType(), True),
            StructField("OffIce_A_unblockedShotAttempts", IntegerType(), True),
            StructField("OffIce_A_goals", IntegerType(), True),
            # Possession Metrics
            StructField("corsiFor", IntegerType(), True),
            StructField("corsiAgainst", IntegerType(), True),
            StructField("fenwickFor", IntegerType(), True),
            StructField("fenwickAgainst", IntegerType(), True),
            # Zone Entry/Exit
            StructField("zoneEntriesFor", IntegerType(), True),
            StructField("zoneEntriesAgainst", IntegerType(), True),
            StructField("continuedInZoneFor", IntegerType(), True),
            StructField("continuedInZoneAgainst", IntegerType(), True),
            StructField("continuedOutOfZoneFor", IntegerType(), True),
            StructField("continuedOutOfZoneAgainst", IntegerType(), True),
            # Penalties
            StructField("penaltiesFor", IntegerType(), True),
            StructField("penaltiesAgainst", IntegerType(), True),
            StructField("penalityMinutesFor", DoubleType(), True),
            StructField("penalityMinutesAgainst", DoubleType(), True),
            # Faceoffs
            StructField("faceOffsWonFor", IntegerType(), True),
            StructField("faceOffsLostFor", IntegerType(), True),
            # Hits & Takeaways
            StructField("hitsFor", IntegerType(), True),
            StructField("hitsAgainst", IntegerType(), True),
            StructField("takeawaysFor", IntegerType(), True),
            StructField("takeawaysAgainst", IntegerType(), True),
            StructField("giveawaysFor", IntegerType(), True),
            StructField("giveawaysAgainst", IntegerType(), True),
            # Expected Goals (placeholders for future enhancement)
            StructField("xGoalsFor", DoubleType(), True),
            StructField("xGoalsAgainst", DoubleType(), True),
            # Calculated Percentages
            StructField("corsiPercentage", DoubleType(), True),
            StructField("fenwickPercentage", DoubleType(), True),
            StructField("onIce_corsiPercentage", DoubleType(), True),
            StructField("offIce_corsiPercentage", DoubleType(), True),
            StructField("onIce_fenwickPercentage", DoubleType(), True),
            StructField("offIce_fenwickPercentage", DoubleType(), True),
        ]
    )


def get_games_historical_schema():
    """Returns schema for bronze_games_historical_v2 table."""
    return StructType(
        [
            # Game identifiers
            StructField("gameId", IntegerType(), False),
            StructField("season", IntegerType(), True),
            StructField("gameDate", IntegerType(), True),  # YYYYMMDD
            StructField("team", StringType(), True),
            StructField("opposingTeam", StringType(), True),
            StructField(
                "playerTeam", StringType(), True
            ),  # Same as team, for compatibility
            StructField("home_or_away", StringType(), True),  # "HOME" or "AWAY"
            StructField(
                "situation", StringType(), False
            ),  # "all", "5on4", "4on5", "5on5"
            StructField("playoffGame", IntegerType(), True),  # 0 or 1
            # Shots and Goals
            StructField("shotsOnGoalFor", IntegerType(), True),
            StructField("shotsOnGoalAgainst", IntegerType(), True),
            StructField("goalsFor", IntegerType(), True),
            StructField("goalsAgainst", IntegerType(), True),
            # Shot attempts (Corsi)
            StructField("missedShotsFor", IntegerType(), True),
            StructField("missedShotsAgainst", IntegerType(), True),
            StructField("blockedShotAttemptsFor", IntegerType(), True),
            StructField("blockedShotAttemptsAgainst", IntegerType(), True),
            StructField("shotAttemptsFor", IntegerType(), True),  # Calculated
            StructField("shotAttemptsAgainst", IntegerType(), True),  # Calculated
            StructField("unblockedShotAttemptsFor", IntegerType(), True),  # For Fenwick
            StructField(
                "unblockedShotAttemptsAgainst", IntegerType(), True
            ),  # For Fenwick
            # Saved shots
            StructField("savedShotsOnGoalFor", IntegerType(), True),
            StructField("savedShotsOnGoalAgainst", IntegerType(), True),
            StructField("savedUnblockedShotAttemptsFor", IntegerType(), True),
            StructField("savedUnblockedShotAttemptsAgainst", IntegerType(), True),
            # Rebounds
            StructField("reboundsFor", IntegerType(), True),
            StructField("reboundGoalsFor", IntegerType(), True),
            StructField("reboundsAgainst", IntegerType(), True),
            StructField("reboundGoalsAgainst", IntegerType(), True),
            # Zone continuations
            StructField("playContinuedInZoneFor", IntegerType(), True),
            StructField("playContinuedOutsideZoneFor", IntegerType(), True),
            StructField("playContinuedInZoneAgainst", IntegerType(), True),
            StructField("playContinuedOutsideZoneAgainst", IntegerType(), True),
            # Penalties and Faceoffs
            StructField("penaltiesFor", IntegerType(), True),
            StructField("penaltiesAgainst", IntegerType(), True),
            StructField("faceOffsWonFor", IntegerType(), True),
            StructField("faceOffsWonAgainst", IntegerType(), True),
            # Hits, Takeaways, Giveaways
            StructField("hitsFor", IntegerType(), True),
            StructField("hitsAgainst", IntegerType(), True),
            StructField("takeawaysFor", IntegerType(), True),
            StructField("takeawaysAgainst", IntegerType(), True),
            StructField("giveawaysFor", IntegerType(), True),
            StructField("giveawaysAgainst", IntegerType(), True),
            # Shot danger (For)
            StructField("lowDangerShotsFor", IntegerType(), True),
            StructField("mediumDangerShotsFor", IntegerType(), True),
            StructField("highDangerShotsFor", IntegerType(), True),
            StructField("lowDangerGoalsFor", IntegerType(), True),
            StructField("mediumDangerGoalsFor", IntegerType(), True),
            StructField("highDangerGoalsFor", IntegerType(), True),
            # Shot danger (Against)
            StructField("lowDangerShotsAgainst", IntegerType(), True),
            StructField("mediumDangerShotsAgainst", IntegerType(), True),
            StructField("highDangerShotsAgainst", IntegerType(), True),
            StructField("lowDangerGoalsAgainst", IntegerType(), True),
            StructField("mediumDangerGoalsAgainst", IntegerType(), True),
            StructField("highDangerGoalsAgainst", IntegerType(), True),
            # Calculated Percentages
            StructField("corsiPercentage", DoubleType(), True),
            StructField("fenwickPercentage", DoubleType(), True),
            # Expected Goals
            StructField("xGoalsFor", DoubleType(), True),
            StructField("xGoalsAgainst", DoubleType(), True),
        ]
    )


def get_skaters_schema():
    """Returns schema for bronze_skaters_2023_v2 table."""
    return StructType(
        [
            StructField("playerId", StringType(), False),
            StructField("name", StringType(), True),
            StructField("team", StringType(), True),
            StructField("position", StringType(), True),
            StructField("season", IntegerType(), True),
            StructField("situation", StringType(), False),
            # Aggregated stats (same columns as player_game_stats but aggregated)
            # Will be calculated by summing player_game_stats across all games
            StructField("player_icetime", DoubleType(), True),
            StructField("player_I_F_goals", IntegerType(), True),
            StructField("player_I_F_shotsOnGoal", IntegerType(), True),
            StructField("player_I_F_shotAttempts", IntegerType(), True),
            # ... (add more aggregated stats as needed)
        ]
    )


# COMMAND ----------

# MAGIC %md
# MAGIC ### Bronze Layer Tables

# COMMAND ----------

# MAGIC %md
# MAGIC #### üîÑ STREAMING ARCHITECTURE: Append-Only Bronze Tables
# MAGIC
# MAGIC This uses DLT's streaming table + append flow pattern to ensure data is NEVER lost on code changes.
# MAGIC
# MAGIC **How it works:**
# MAGIC 1. `dlt.create_streaming_table()` creates the target table (persists forever, even with code changes)
# MAGIC 2. `@dlt.append_flow()` appends new data to the streaming table
# MAGIC 3. Data is deduplicated before appending (no duplicates!)
# MAGIC 4. Incremental runs only fetch data from today onwards (fast!)
# MAGIC
# MAGIC **Benefits:**
# MAGIC - ‚úÖ Code changes DO NOT trigger table drops
# MAGIC - ‚úÖ Historical data is protected
# MAGIC - ‚úÖ True incremental append (not full refresh)
# MAGIC - ‚úÖ Deduplication built-in

# COMMAND ----------


# ==============================================================================
# STAGING PATTERN: API Ingestion ‚Üí Staging Table ‚Üí Streaming Table
# ==============================================================================
# This pattern provides:
# 1. Fast incremental API calls (5-10 min) via staging table
# 2. Data protection (streaming table never drops) via append-only flow
# ==============================================================================


# DBTITLE 1,bronze_player_game_stats_v2_staging - API Ingestion (Batch)
@dlt.table(
    name="bronze_player_game_stats_v2_staging",
    comment="Staging table for API ingestion - rebuilds quickly with incremental date logic",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py",
        "layer": "staging",
        "pipelines.reset.allowed": "false",  # Protect staging from accidental resets
        "pipelines.autoOptimize.managed": "false",  # Force regular table (not materialized view)
    },
)
def ingest_player_game_stats_staging():
    """
    Fetch player game-by-game statistics from NHL API (STAGING TABLE).

    This table rebuilds on code changes, but is FAST due to incremental date logic:
    - skip_staging_ingestion=true: Use existing staging data (no API calls!)
    - one_time_load=true: Full historical (only on first run)
    - one_time_load=false: Only recent dates (5-10 min API calls)

    The final streaming table reads from this staging table and preserves all history.
    """

    # SKIP MODE: Return empty DataFrame (actual data read from _manual tables by streaming flows)
    if skip_staging_ingestion:
        print("‚è≠Ô∏è  SKIP MODE: Staging function bypassed (manual tables used directly)")
        print("   Streaming flows will read from _staging_manual tables")
        # Return empty DataFrame with correct schema (DLT requires a return value)
        from pyspark.sql.types import StructType

        return spark.createDataFrame([], schema=get_player_game_stats_schema())

    print(f"üèí STAGING: NHL API ingestion: {start_date} to {end_date}")

    # Generate date range
    date_list = generate_date_range(start_date, end_date)
    print(f"üìÖ Processing {len(date_list)} dates")

    all_player_stats = []
    games_processed = 0

    for date_str in date_list:
        print(f"\nüìÖ Processing date: {date_str}")

        try:
            # Fetch schedule for this date
            schedule = fetch_with_retry(
                nhl_client, nhl_client.schedule.daily_schedule, date=date_str
            )

            if not schedule or "games" not in schedule:
                print(f"  ‚ö†Ô∏è No games found for {date_str}")
                continue

            games = schedule["games"]
            print(f"  üéÆ Found {len(games)} games on {date_str}")

            for game in games:
                game_id = game.get("id")
                home_team = game.get("homeTeam", {})
                away_team = game.get("awayTeam", {})

                print(
                    f"    Processing game {game_id}: {away_team.get('abbrev')} @ {home_team.get('abbrev')}"
                )

                try:
                    # Fetch play-by-play data
                    pbp = fetch_with_retry(
                        nhl_client, nhl_client.game_center.play_by_play, game_id=game_id
                    )

                    # Fetch shift data
                    shift_response = fetch_with_retry(
                        nhl_client,
                        nhl_client.game_center.shift_chart_data,
                        game_id=game_id,
                    )

                    # Extract shifts from nested response
                    shifts = (
                        shift_response.get("data", [])
                        if isinstance(shift_response, dict)
                        else shift_response
                    )

                    # Fetch boxscore for team IDs
                    boxscore = fetch_with_retry(
                        nhl_client, nhl_client.game_center.boxscore, game_id=game_id
                    )

                    if not pbp or not shifts or not boxscore:
                        missing = []
                        if not pbp:
                            missing.append("play-by-play")
                        if not shifts:
                            missing.append("shifts")
                        if not boxscore:
                            missing.append("boxscore")
                        print(
                            f"      ‚ö†Ô∏è Missing data for game {game_id}: {', '.join(missing)}"
                        )
                        continue

                    # Get rosters from boxscore
                    # Note: playerByGameStats has stats but may not have full names
                    # We need to combine with roster data for names
                    home_players = boxscore.get("playerByGameStats", {}).get(
                        "homeTeam", {}
                    ).get("forwards", []) + boxscore.get("playerByGameStats", {}).get(
                        "homeTeam", {}
                    ).get(
                        "defense", []
                    )
                    away_players = boxscore.get("playerByGameStats", {}).get(
                        "awayTeam", {}
                    ).get("forwards", []) + boxscore.get("playerByGameStats", {}).get(
                        "awayTeam", {}
                    ).get(
                        "defense", []
                    )

                    # Build player ID to name mapping from play-by-play roster
                    # This has the actual name data
                    player_names = {}
                    if "rosterSpots" in pbp:
                        for roster_player in pbp.get("rosterSpots", []):
                            player_id = str(roster_player.get("playerId"))
                            first_name = roster_player.get("firstName", {})
                            last_name = roster_player.get("lastName", {})

                            # Handle both dict and string formats
                            if isinstance(first_name, dict):
                                first_name = first_name.get("default", "")
                            elif not isinstance(first_name, str):
                                first_name = ""

                            if isinstance(last_name, dict):
                                last_name = last_name.get("default", "")
                            elif not isinstance(last_name, str):
                                last_name = ""

                            full_name = f"{first_name} {last_name}".strip()
                            if full_name:
                                player_names[player_id] = full_name

                    # Get team IDs from boxscore (for play-by-play matching)
                    home_team_id = boxscore.get("homeTeam", {}).get("id")
                    away_team_id = boxscore.get("awayTeam", {}).get("id")

                    # Process home team players
                    for player in home_players:
                        player_id = str(player.get("playerId"))
                        # Look up name from rosterSpots (has actual names)
                        player_name = player_names.get(player_id, "")
                        position = player.get("position", None)

                        # Aggregate stats for this player using helper function
                        player_stats = aggregate_player_stats_by_situation(
                            pbp_data=pbp,
                            shift_data=shifts,
                            player_id=player_id,
                            team_id=home_team_id,
                            team_side="home",
                            player_name=player_name,
                            player_team=home_team.get("abbrev"),
                            opposing_team=away_team.get("abbrev"),
                            position=position,
                            is_home=True,
                            game_id=game_id,
                            game_date=int(date_str.replace("-", "")),  # YYYYMMDD
                            season=game.get("season"),
                        )

                        all_player_stats.extend(player_stats)

                    # Process away team players
                    for player in away_players:
                        player_id = str(player.get("playerId"))
                        # Look up name from rosterSpots (has actual names)
                        player_name = player_names.get(player_id, "")
                        position = player.get("position", None)

                        player_stats = aggregate_player_stats_by_situation(
                            pbp_data=pbp,
                            shift_data=shifts,
                            player_id=player_id,
                            team_id=away_team_id,
                            team_side="away",
                            player_name=player_name,
                            player_team=away_team.get("abbrev"),
                            opposing_team=home_team.get("abbrev"),
                            position=position,
                            is_home=False,
                            game_id=game_id,
                            game_date=int(date_str.replace("-", "")),
                            season=game.get("season"),
                        )

                        all_player_stats.extend(player_stats)

                    games_processed += 1
                    print(f"      ‚úÖ Game {game_id} processed successfully")

                except Exception as e:
                    import traceback

                    print(f"      ‚ùå Error processing game {game_id}: {str(e)}")
                    print(f"      Traceback: {traceback.format_exc()}")
                    continue

        except Exception as e:
            print(f"  ‚ùå Error processing date {date_str}: {str(e)}")
            continue

    print(
        f"\n‚úÖ Ingestion complete: {games_processed} games processed, {len(all_player_stats)} player-game records"
    )

    # Convert to Spark DataFrame
    if not all_player_stats:
        print("‚ö†Ô∏è No data collected, returning empty DataFrame")
        return spark.createDataFrame([], schema=get_player_game_stats_schema())

    df = spark.createDataFrame(all_player_stats, schema=get_player_game_stats_schema())
    print(f"üìä DataFrame created: {df.count()} rows, {len(df.columns)} columns")

    # Deduplicate records - keep most recent for each (playerId, gameId, situation)
    # This prevents duplicate key errors in downstream models
    initial_count = df.count()
    df_deduped = df.dropDuplicates(["playerId", "gameId", "situation"])
    final_count = df_deduped.count()

    duplicates_removed = initial_count - final_count
    if duplicates_removed > 0:
        print(f"üßπ Deduplication: Removed {duplicates_removed} duplicate records")
        print(f"   Keys: (playerId, gameId, situation)")
        print(f"   Final count: {final_count} unique records")
    else:
        print(f"‚úÖ No duplicates found - all {final_count} records are unique")

    return df_deduped


# COMMAND ----------


# DBTITLE 1,bronze_player_game_stats_v2 - Streaming Table (Protected)
dlt.create_streaming_table(
    name="bronze_player_game_stats_v2",
    comment="Player game-by-game stats - PROTECTED streaming table (append-only, never drops)",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py (via staging)",
        "delta.enableChangeDataFeed": "true",
        "pipelines.reset.allowed": "false",
    },
    expect_all_or_drop={
        "playerTeam is not null": "playerTeam IS NOT NULL",
        "season is not null": "season IS NOT NULL",
        "situation is not null": "situation IS NOT NULL",
        "playerId is not null": "playerId IS NOT NULL",
    },
)


# DBTITLE 1,bronze_player_game_stats_v2 - Stream from Staging
@dlt.append_flow(
    target="bronze_player_game_stats_v2",
    comment="Streams from staging table - preserves all historical data",
)
def stream_player_stats_from_staging():
    """
    Stream from staging table to final protected streaming table.

    This ensures:
    - Staging can rebuild quickly (incremental API calls)
    - Final table accumulates all history (never drops)
    - Code changes only affect staging (5-10 min rebuild)
    """
    if skip_staging_ingestion:
        # Skip mode: Read from _staging_manual table (like successful run last night)
        print("‚è≠Ô∏è  STREAMING: Reading from _staging_manual table")
        return spark.readStream.option("skipChangeCommits", "true").table(
            f"{catalog}.{schema}.bronze_player_game_stats_v2_staging_manual"
        )
    else:
        # Normal mode: Use dlt.read_stream() to establish dependency on staging table
        print("‚è≠Ô∏è  STREAMING: Reading from DLT staging table")
        return dlt.read_stream("bronze_player_game_stats_v2_staging")


# COMMAND ----------


# DBTITLE 1,bronze_games_historical_v2_staging - API Ingestion (Batch)
@dlt.table(
    name="bronze_games_historical_v2_staging",
    comment="Staging table for team game stats - rebuilds quickly with incremental date logic",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py",
        "layer": "staging",
        "pipelines.reset.allowed": "false",  # Protect staging from accidental resets
        "pipelines.autoOptimize.managed": "false",  # Force regular table (not materialized view)
    },
)
def ingest_games_historical_staging():
    """
    Fetch team-level game statistics from NHL API (STAGING TABLE).

    This table rebuilds on code changes, but is FAST due to incremental date logic:
    - skip_staging_ingestion=true: Use existing staging data (no API calls!)
    - one_time_load=true: Full historical (only on first run)
    - one_time_load=false: Only recent dates (5-10 min API calls)

    The final streaming table reads from this staging table and preserves all history.
    """

    # SKIP MODE: Return empty DataFrame (actual data read from _manual tables by streaming flows)
    if skip_staging_ingestion:
        print("‚è≠Ô∏è  SKIP MODE: Staging function bypassed (manual tables used directly)")
        print("   Streaming flows will read from _staging_manual tables")
        # Return empty DataFrame with correct schema (DLT requires a return value)
        return spark.createDataFrame([], schema=get_games_historical_schema())

    print(f"üèí STAGING: Team game stats ingestion: {start_date} to {end_date}")

    date_list = generate_date_range(start_date, end_date)
    all_team_stats = []

    for date_str in date_list:
        print(f"\nüìÖ Processing date: {date_str}")

        try:
            schedule = fetch_with_retry(
                nhl_client, nhl_client.schedule.daily_schedule, date=date_str
            )

            if not schedule or "games" not in schedule:
                continue

            games = schedule["games"]

            for game in games:
                game_id = game.get("id")
                home_team = game.get("homeTeam", {})
                away_team = game.get("awayTeam", {})
                season = game.get("season")
                game_date = int(date_str.replace("-", ""))
                is_playoff = game.get("gameType", 2) == 3  # Type 3 = playoffs

                print(f"    Processing game {game_id}")

                try:
                    # Fetch data
                    pbp = fetch_with_retry(
                        nhl_client, nhl_client.game_center.play_by_play, game_id=game_id
                    )
                    boxscore = fetch_with_retry(
                        nhl_client, nhl_client.game_center.boxscore, game_id=game_id
                    )

                    if not pbp or not boxscore:
                        continue

                    # Get team IDs from boxscore
                    home_team_id = boxscore.get("homeTeam", {}).get("id")
                    away_team_id = boxscore.get("awayTeam", {}).get("id")

                    # Aggregate stats for home team
                    home_stats_by_sit = aggregate_team_stats_by_situation(
                        pbp, home_team.get("abbrev"), home_team_id, is_home=True
                    )

                    # Aggregate stats for away team
                    away_stats_by_sit = aggregate_team_stats_by_situation(
                        pbp, away_team.get("abbrev"), away_team_id, is_home=False
                    )

                    # Create records for each situation
                    for situation in ["all", "5on4", "4on5", "5on5"]:
                        # Home team record
                        home_record = {
                            "gameId": game_id,
                            "season": season,
                            "gameDate": game_date,
                            "team": home_team.get("abbrev"),
                            "playerTeam": home_team.get("abbrev"),  # For compatibility
                            "opposingTeam": away_team.get("abbrev"),
                            "home_or_away": "HOME",
                            "situation": situation,
                            "playoffGame": 1 if is_playoff else 0,
                            **home_stats_by_sit.get(situation, {}),
                        }

                        # Away team record
                        away_record = {
                            "gameId": game_id,
                            "season": season,
                            "gameDate": game_date,
                            "team": away_team.get("abbrev"),
                            "playerTeam": away_team.get("abbrev"),  # For compatibility
                            "opposingTeam": home_team.get("abbrev"),
                            "home_or_away": "AWAY",
                            "situation": situation,
                            "playoffGame": 1 if is_playoff else 0,
                            **away_stats_by_sit.get(situation, {}),
                        }

                        all_team_stats.append(home_record)
                        all_team_stats.append(away_record)

                    print(f"      ‚úÖ Game {game_id} processed")

                except Exception as e:
                    print(f"      ‚ùå Error: {str(e)}")
                    continue

        except Exception as e:
            print(f"  ‚ùå Error processing date {date_str}: {str(e)}")
            continue

    print(f"\n‚úÖ Team stats complete: {len(all_team_stats)} records")

    if not all_team_stats:
        return spark.createDataFrame([], schema=get_games_historical_schema())

    df = spark.createDataFrame(all_team_stats, schema=get_games_historical_schema())

    # Deduplicate records - keep most recent for each (gameId, team, situation)
    # This prevents duplicate key errors in downstream models
    initial_count = df.count()
    df_deduped = df.dropDuplicates(["gameId", "team", "situation"])
    final_count = df_deduped.count()

    duplicates_removed = initial_count - final_count
    if duplicates_removed > 0:
        print(f"üßπ Deduplication: Removed {duplicates_removed} duplicate records")
        print(f"   Keys: (gameId, team, situation)")
        print(f"   Final count: {final_count} unique records")
    else:
        print(f"‚úÖ No duplicates found - all {final_count} records are unique")

    return df_deduped


# COMMAND ----------


# DBTITLE 1,bronze_games_historical_v2 - Streaming Table (Protected)
dlt.create_streaming_table(
    name="bronze_games_historical_v2",
    comment="Team game statistics - PROTECTED streaming table (append-only, never drops)",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py (via staging)",
        "delta.enableChangeDataFeed": "true",
        "pipelines.reset.allowed": "false",
    },
    expect_all_or_drop={
        "gameId is not null": "gameId IS NOT NULL",
    },
)


# DBTITLE 1,bronze_games_historical_v2 - Stream from Staging
@dlt.append_flow(
    target="bronze_games_historical_v2",
    comment="Streams from staging table - preserves all historical data",
)
def stream_games_from_staging():
    """
    Stream from staging table to final protected streaming table.

    This ensures:
    - Staging can rebuild quickly (incremental API calls)
    - Final table accumulates all history (never drops)
    - Code changes only affect staging (5-10 min rebuild)
    """
    if skip_staging_ingestion:
        # Skip mode: Read from _staging_manual table (like successful run last night)
        print("‚è≠Ô∏è  STREAMING: Reading from _staging_manual table")
        return spark.readStream.option("skipChangeCommits", "true").table(
            f"{catalog}.{schema}.bronze_games_historical_v2_staging_manual"
        )
    else:
        # Normal mode: Use dlt.read_stream() to establish dependency on staging table
        print("‚è≠Ô∏è  STREAMING: Reading from DLT staging table")
        return dlt.read_stream("bronze_games_historical_v2_staging")


# COMMAND ----------


# DBTITLE 1,bronze_schedule_2023_v2 - Derived Table
@dlt.table(
    name="bronze_schedule_2023_v2",
    comment="NHL schedule derived from games_historical (rebuilds when source changes)",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py (derived)",
        "delta.enableChangeDataFeed": "true",
        "pipelines.reset.allowed": "false",
    },
)
def ingest_schedule_v2():
    """
    Fetch NHL schedule including FUTURE games from NHL API.

    This combines:
    1. Historical games (derived from games_historical)
    2. Future games (fetched from NHL schedule API)

    This ensures we have upcoming games for ML predictions!
    """
    from pyspark.sql.functions import (
        col,
        date_format,
        when,
        first,
        to_date,
        lit,
        coalesce,
    )
    from datetime import datetime, timedelta

    print(f"üìÖ Fetching NHL schedule (historical + future games)")

    # PART 1: Get historical games from games_historical
    if skip_staging_ingestion:
        print(f"   üìä Historical: Reading from _staging_manual table")
        games_df = spark.table(
            f"{catalog}.{schema}.bronze_games_historical_v2_staging_manual"
        )
    else:
        print(f"   üìä Historical: Reading from final streaming table")
        games_df = dlt.read("bronze_games_historical_v2")

    # Derive historical schedule
    historical_schedule = games_df.groupBy("gameId", "gameDate").agg(
        first(when(col("home_or_away") == "HOME", col("team"))).alias("HOME"),
        first(when(col("home_or_away") == "AWAY", col("team"))).alias("AWAY"),
    )

    historical_schedule = (
        historical_schedule.withColumn("gameDate_str", col("gameDate").cast("string"))
        .withColumn("DATE", to_date(col("gameDate_str"), "yyyyMMdd"))
        .withColumn("DAY", date_format(col("DATE"), "E."))
        .withColumn("EASTERN", lit("7:00 PM"))
        .withColumn("LOCAL", lit("7:00 PM"))
        .withColumn("GAME_ID", col("gameId"))
        .select("GAME_ID", "DAY", "DATE", "EASTERN", "LOCAL", "AWAY", "HOME")
    )

    print(f"   ‚úÖ Historical schedule: {historical_schedule.count()} games")

    # PART 2: Fetch FUTURE games from NHL schedule API
    # Config: schedule_future_days (default 8 = today + 7 ahead) so gold has upcoming NHL games.
    schedule_future_days = int(spark.conf.get("schedule_future_days", "8"))
    print(f"   üìÖ Fetching future schedule from NHL API (next {schedule_future_days} days)...")

    future_games = []
    for i in range(0, schedule_future_days):
        future_date = today + timedelta(days=i)
        date_str = future_date.strftime("%Y-%m-%d")

        try:
            schedule = fetch_with_retry(
                nhl_client, nhl_client.schedule.daily_schedule, date=date_str
            )

            if schedule and "games" in schedule:
                for game in schedule["games"]:
                    game_id = game.get("id")
                    game_date = game.get("gameDate", date_str)

                    home_team = game.get("homeTeam", {})
                    away_team = game.get("awayTeam", {})

                    # Parse date
                    try:
                        game_date_obj = datetime.strptime(
                            game_date[:10], "%Y-%m-%d"
                        ).date()
                    except:
                        game_date_obj = future_date

                    future_games.append(
                        {
                            "GAME_ID": game_id,
                            "DAY": game_date_obj.strftime("%a."),
                            "DATE": game_date_obj,
                            "EASTERN": "7:00 PM",
                            "LOCAL": "7:00 PM",
                            "AWAY": away_team.get("abbrev", "UNK"),
                            "HOME": home_team.get("abbrev", "UNK"),
                        }
                    )
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error fetching schedule for {date_str}: {str(e)}")
            continue

    print(f"   ‚úÖ Future schedule: {len(future_games)} games")

    # PART 3: Combine historical + future
    if future_games:
        from pyspark.sql.types import (
            StructType,
            StructField,
            StringType,
            DateType,
            IntegerType,
        )

        schedule_schema = StructType(
            [
                StructField("GAME_ID", IntegerType(), True),
                StructField("DAY", StringType(), True),
                StructField("DATE", DateType(), True),
                StructField("EASTERN", StringType(), True),
                StructField("LOCAL", StringType(), True),
                StructField("AWAY", StringType(), True),
                StructField("HOME", StringType(), True),
            ]
        )

        future_schedule = spark.createDataFrame(future_games, schema=schedule_schema)

        # Union historical + future
        combined_schedule = historical_schedule.unionByName(future_schedule)
        print(
            f"   ‚úÖ Combined schedule: {combined_schedule.count()} total games (historical + future)"
        )
    else:
        print(f"   ‚ö†Ô∏è  No future games found, using historical only")
        combined_schedule = historical_schedule

    # Deduplicate
    final_schedule = combined_schedule.dropDuplicates(["GAME_ID"])
    print(f"   ‚úÖ Final schedule: {final_schedule.count()} unique games")

    return final_schedule


# COMMAND ----------


# DBTITLE 1,bronze_skaters_2023_v2 - Derived Table
@dlt.table(
    name="bronze_skaters_2023_v2",
    comment="Aggregated skater statistics derived from player_game_stats (rebuilds when source changes)",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py (derived)",
        "delta.enableChangeDataFeed": "true",
        "pipelines.reset.allowed": "false",
    },
)
def ingest_skaters_v2():
    """
    Derive skater aggregate stats from player_game_stats.

    This is a derived table that rebuilds when the source changes.
    Uses batch read for complex aggregations.
    """
    print("üìä Aggregating skater stats from player_game_stats")

    # Read from appropriate source based on skip mode
    if skip_staging_ingestion:
        # Skip mode: Read from MANUAL staging table (different name to avoid DLT ownership)
        print(f"   ‚è≠Ô∏è  Reading from manual staging table (_staging_manual)")
        player_stats = spark.table(
            f"{catalog}.{schema}.bronze_player_game_stats_v2_staging_manual"
        )
    else:
        # Normal mode: final table has accumulated historical data
        print(f"   üìä Reading from final streaming table (normal mode)")
        player_stats = dlt.read("bronze_player_game_stats_v2")

    # Aggregate by player, team, season, situation
    skaters_df = (
        player_stats.groupBy(
            "playerId", "name", "playerTeam", "position", "season", "situation"
        )
        .agg(
            sum("icetime").alias("player_icetime"),
            sum("I_F_goals").alias("player_I_F_goals"),
            sum("I_F_shotsOnGoal").alias("player_I_F_shotsOnGoal"),
            sum("I_F_shotAttempts").alias("player_I_F_shotAttempts"),
            # Add more aggregations as needed
        )
        .withColumnRenamed("playerTeam", "team")
    )

    # Deduplicate by (playerId, season, situation) - should already be unique from groupBy
    initial_count = skaters_df.count()
    skaters_df = skaters_df.dropDuplicates(["playerId", "season", "situation"])
    final_count = skaters_df.count()

    if initial_count != final_count:
        print(
            f"üßπ Skaters deduplication: Removed {initial_count - final_count} duplicates"
        )
        print(f"   Final count: {final_count} unique player-situation combinations")
    else:
        print(
            f"‚úÖ Skaters aggregated: {final_count} unique player-situation combinations"
        )

    return skaters_df


# COMMAND ----------

# MAGIC %md
# MAGIC ### Testing and Validation

# COMMAND ----------

# DBTITLE 1,Data Quality Checks
# These run automatically via @dlt.expect_or_drop decorators
# Additional custom quality checks can be added here

print(
    """
‚úÖ Bronze layer ingestion configured successfully!

üìã Tables created:
1. bronze_player_game_stats_v2_nhl_api - Player game-by-game stats
2. bronze_games_historical_v2_nhl_api - Team game stats
3. bronze_schedule_2023_v2_nhl_api - NHL schedule (using existing)
4. bronze_skaters_2023_v2_nhl_api - Aggregated player stats

üéØ Schema compatibility: 100% MoneyPuck-compatible
üîÑ Downstream impact: ZERO changes required

üöÄ Ready to run in DLT pipeline!
"""
)
