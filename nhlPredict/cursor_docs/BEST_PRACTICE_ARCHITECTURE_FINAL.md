# NHL SOG Prediction Pipeline - Best Practice Architecture

**Date:** 2026-02-04  
**Version:** 1.0 - Final Recommended Architecture  
**Status:** Ready for Implementation

---

## üéØ Business Requirements

### Primary Goals
1. **ML Predictions:** Predict shots on goal for upcoming NHL games
2. **ML Training:** Train models on 2+ seasons of historical data
3. **Daily Updates:** Run incrementally every day (5-10 minutes, not hours)
4. **Data Protection:** No data loss on code changes or deployments
5. **Fast Recovery:** Use backups for migrations (avoid 4-5 hour API reloads)

### Data Characteristics
- **Historical Data:** 492K+ player-game records (Oct 2023 - Present)
- **Incremental Data:** ~15-20 games/day (~1,800 player-game records/day)
- **API Source:** NHL API (batch REST calls, not streaming)
- **Update Frequency:** Daily (overnight or morning runs)
- **Prediction Window:** Next 7 days of upcoming games

---

## üèóÔ∏è Recommended Architecture (Best Practice)

### Option A: Simplified Append-Only with Smart Dedup (RECOMMENDED)

This is **Databricks' recommended pattern** for batch API ingestion with data protection.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NHL API (Batch REST)                     ‚îÇ
‚îÇ               Historical + Future Schedule                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   @dlt.table() - Bronze                     ‚îÇ
‚îÇ              with Smart Incremental Logic                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚Ä¢ Queries MAX(gameDate) from existing table                ‚îÇ
‚îÇ  ‚Ä¢ Only fetches new dates from API (5-10 min)              ‚îÇ
‚îÇ  ‚Ä¢ Even on rebuild, only fetches recent data               ‚îÇ
‚îÇ  ‚Ä¢ Deduplicates on write                                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Table Properties:                                          ‚îÇ
‚îÇ    - pipelines.reset.allowed: false                         ‚îÇ
‚îÇ    - delta.enableChangeDataFeed: true                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   Silver/Gold Layers
                   (Standard @dlt.table())
```

#### Why This is Best Practice

**Pros:**
- ‚úÖ **Simple:** Standard DLT pattern, no staging complexity
- ‚úÖ **Fast rebuilds:** Smart incremental logic (5-10 min, not 4-5 hours)
- ‚úÖ **Data protection:** `pipelines.reset.allowed: false` + manual backups
- ‚úÖ **Works with batch APIs:** No streaming conversion needed
- ‚úÖ **Easy debugging:** Standard Spark DataFrames
- ‚úÖ **Low storage:** Single table per source
- ‚úÖ **Databricks recommended:** Most common production pattern

**Cons:**
- ‚ö†Ô∏è **Code changes rebuild table:** But fast (5-10 min) due to incremental logic
- ‚ö†Ô∏è **Requires backup strategy:** Manual recovery if something goes wrong

**When Code Changes Happen:**
1. DLT detects code change ‚Üí triggers table rebuild
2. Function runs with incremental logic ‚Üí only fetches last 1-2 days from API (5-10 min)
3. Historical data lost from table BUT preserved in Delta history (time travel)
4. Manual recovery: Set `one_time_load: true` or restore from backup

**Why This is Acceptable:**
- Code changes are infrequent (weekly/monthly, not daily)
- 5-10 min rebuild is fast enough for occasional recovery
- Delta time travel provides 30-day retention
- Regular backups provide long-term recovery

---

### Option B: Staging Pattern with Streaming Tables (COMPLEX)

This is what we've been trying to implement - works but has significant complexity.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NHL API (Batch REST)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           @dlt.table() - Staging (Rebuilds)                 ‚îÇ
‚îÇ                 bronze_xxx_staging                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚Ä¢ Fetches from API with incremental logic                  ‚îÇ
‚îÇ  ‚Ä¢ Fast rebuild (5-10 min)                                  ‚îÇ
‚îÇ  ‚Ä¢ Returns batch DataFrame                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      @dlt.append_flow() ‚Üí STREAMING_TABLE (Protected)      ‚îÇ
‚îÇ                   bronze_xxx_final                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚Ä¢ Reads from staging via spark.readStream                  ‚îÇ
‚îÇ  ‚Ä¢ Uses skipChangeCommits: true                             ‚îÇ
‚îÇ  ‚Ä¢ Append-only (never drops)                                ‚îÇ
‚îÇ  ‚Ä¢ Table Properties: pipelines.reset.allowed: false         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   Silver/Gold Layers
```

#### Why This is More Complex

**Pros:**
- ‚úÖ **Maximum data protection:** Final tables NEVER drop
- ‚úÖ **True append-only:** Streaming table semantics
- ‚úÖ **Code changes safe:** Only staging rebuilds (5-10 min), final accumulates

**Cons:**
- ‚ùå **Complex:** 2x tables per source (staging + final)
- ‚ùå **More storage:** ~2x cost (staging + final + checkpoints)
- ‚ùå **Streaming issues:** Checkpoint conflicts, skipChangeCommits needed
- ‚ùå **Harder debugging:** Streaming errors more cryptic
- ‚ùå **Ownership conflicts:** Careful table naming required

**Implementation Challenges:**
1. Streaming flows need `skipChangeCommits: true` for staging overwrites
2. Checkpoint management can be tricky
3. Table ownership conflicts during migrations
4. More moving parts = more failure modes

---

## üéØ Recommended Solution: Option A (Simplified)

### Why Choose Simplicity

**Your use case:**
- Daily batch updates (not real-time streaming)
- Code changes are infrequent (not multiple times per day)
- 5-10 min rebuild is acceptable for occasional code changes
- Storage efficiency matters (demo/dev environment)

**Option A provides:**
- 90% of the protection with 10% of the complexity
- Fast incremental runs (5-10 min)
- Standard DLT pattern (well-documented, widely used)
- Easy to understand and maintain

**Delta Time Travel Safety Net:**
- 30 days of table history
- Can recover from any rebuild in last month
- Regular backups provide longer-term recovery

---

## üìã Complete Architecture Design (Option A)

### Bronze Layer (4 Tables)

#### 1. `bronze_player_game_stats_v2` (Core Data)

```python
@dlt.table(
    name="bronze_player_game_stats_v2",
    comment="Player game-by-game stats from NHL API (incremental with smart date logic)",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py",
        "delta.enableChangeDataFeed": "true",
        "pipelines.reset.allowed": "false",
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.autoOptimize.autoCompact": "true",
    },
)
def ingest_player_game_stats_v2():
    """
    Fetch player stats from NHL API with smart incremental logic.
    
    Date Logic:
    - one_time_load=true: Oct 2023 ‚Üí today (full historical)
    - one_time_load=false: MAX(gameDate) - lookback ‚Üí today (incremental)
    
    Even on table rebuild, only fetches recent dates!
    """
    
    # Smart incremental logic
    one_time_load = spark.conf.get("one_time_load", "false").lower() == "true"
    lookback_days = int(spark.conf.get("lookback_days", "1"))
    
    if one_time_load:
        start_date = datetime(2023, 10, 1).date()  # Full historical
    else:
        # Query existing table for last date (safe - not in pipeline execution)
        try:
            max_date = spark.sql("""
                SELECT MAX(gameDate) FROM lr_nhl_demo.dev.bronze_player_game_stats_v2
            """).first()[0]
            
            if max_date:
                start_date = datetime.strptime(str(max_date), "%Y%m%d").date() - timedelta(days=lookback_days)
            else:
                start_date = datetime(2023, 10, 1).date()
        except:
            # Table doesn't exist yet - use fallback
            start_date = datetime.now().date() - timedelta(days=lookback_days)
    
    end_date = datetime.now().date()
    
    # Fetch from API (your existing logic)
    df = fetch_player_stats_from_api(start_date, end_date)
    
    # Deduplicate BEFORE returning
    df_deduped = df.dropDuplicates(["playerId", "gameId", "situation"])
    
    return df_deduped
```

**Key Features:**
- Smart incremental: queries table only ONCE at start of function
- Fast rebuilds: only fetches recent dates even if table rebuilds
- No streaming complexity
- Standard batch DataFrame return

#### 2. `bronze_games_historical_v2` (Core Data)

Same pattern as above - simple `@dlt.table()` with smart incremental logic.

#### 3. `bronze_schedule_2023_v2` (Includes Future Games!)

**CRITICAL:** Must fetch future schedule from NHL API for predictions!

```python
@dlt.table(
    name="bronze_schedule_2023_v2",
    comment="NHL schedule: historical (derived) + future (API) for predictions",
    table_properties={
        "quality": "bronze",
        "source": "nhl-api-py + derived",
        "pipelines.reset.allowed": "false",
    },
)
def ingest_schedule_v2():
    """
    Combines historical schedule (from games) + future schedule (from API).
    This ensures we have upcoming games for ML predictions!
    """
    
    # PART 1: Historical games (derive from games_historical)
    games_df = dlt.read("bronze_games_historical_v2")
    historical_schedule = derive_schedule_from_games(games_df)
    
    # PART 2: Future games (fetch from NHL schedule API)
    future_schedule = fetch_future_schedule_from_api(
        start_date=datetime.now().date(),
        end_date=datetime.now().date() + timedelta(days=7)
    )
    
    # PART 3: Combine
    combined_schedule = historical_schedule.unionByName(future_schedule)
    
    # Deduplicate
    return combined_schedule.dropDuplicates(["GAME_ID"])
```

**Why This Matters:**
- Historical schedule: Derived from completed games ‚úÖ
- **Future schedule: Fetched from API** ‚úÖ ‚Üê **CRITICAL FOR PREDICTIONS!**
- Without future games, ML has nothing to predict!

#### 4. `bronze_skaters_2023_v2` (Derived)

Standard derived table - aggregates from player_game_stats.

---

### Silver Layer (3 Tables)

Standard `@dlt.table()` transformations:
- `silver_players_ranked` - Player stats with rolling windows
- `silver_games_schedule_v2` - Cleaned schedule
- `silver_games_rankings` - Team rankings

**No changes needed** - these are transformations, not ingestion.

---

### Gold Layer (4 Tables)

Standard `@dlt.table()` aggregations:
- `gold_player_stats_v2` - Join schedule + players
- `gold_game_stats_v2` - Game-level aggregations
- `gold_merged_stats_v2` - Full feature set
- `gold_model_stats_v2` - ML-ready format

**Key for predictions:** Must read from bronze_schedule that includes future games!

---

## üîÑ Data Flow for ML Predictions

### Historical Data (Training)
```
NHL API (completed games)
    ‚Üì
bronze_player_game_stats_v2 (492K records)
bronze_games_historical_v2 (31K games)
    ‚Üì
silver_players_ranked (123K records with rolling features)
    ‚Üì
gold_model_stats_v2 (123K training records)
    ‚Üì
ML Model Training
```

### Future Data (Predictions)
```
NHL API (schedule endpoint - next 7 days)
    ‚Üì
bronze_schedule_2023_v2 (3955 historical + 20-30 future)
    ‚Üì
silver_games_schedule_v2 (includes future games)
    ‚Üì
gold_player_stats_v2 (joins schedule with player index)
    ‚Üì
gold_model_stats_v2 (300-500 upcoming game predictions)
    ‚îÇ
    ‚îÇ gameDate > today
    ‚îÇ playerId IS NOT NULL (from player index)
    ‚îÇ playerGamesPlayedRolling IS NOT NULL (from historical)
    ‚îÇ player_Total_shotsOnGoal IS NULL (this is what we predict!)
    ‚Üì
ML Model Inference
```

**Critical:** Future games must flow through schedule ‚Üí gold layer with:
- ‚úÖ `playerId` populated (from player roster index)
- ‚úÖ Rolling features populated (from historical player stats)
- ‚úÖ `player_Total_shotsOnGoal` = NULL (target for prediction)

---

## üõ°Ô∏è Data Protection Strategy

### Three-Layer Protection

#### Layer 1: Configuration Protection
```yaml
table_properties:
  pipelines.reset.allowed: "false"  # Prevents manual/API resets
```

**Protects against:**
- ‚úÖ Manual pipeline reset via UI
- ‚úÖ API-triggered reset commands

**Does NOT protect against:**
- ‚ùå Code changes (DLT always rebuilds @dlt.table() on code change)

#### Layer 2: Smart Incremental Logic
```python
# Even if table rebuilds, only fetches recent data!
if not one_time_load:
    start_date = MAX(gameDate) - lookback_days  # Only 1-2 days from API
```

**Protects against:**
- ‚úÖ Expensive full API reloads (5-10 min vs 4-5 hours)
- ‚úÖ Compute cost during rebuilds

**Does NOT protect against:**
- ‚ùå Data loss during rebuild (table drops)

#### Layer 3: Regular Backups
```sql
-- Weekly automated backups
CREATE OR REPLACE TABLE bronze_player_game_stats_v2_backup_latest
DEEP CLONE bronze_player_game_stats_v2;
```

**Protects against:**
- ‚úÖ Data loss from any cause
- ‚úÖ Long-term recovery (beyond 30-day Delta retention)
- ‚úÖ Fast migration/rollback

**Recovery time:** 5-10 minutes (restore from backup)

### Combined Protection Model

**Normal incremental run:** No issues, fast execution ‚úÖ

**Code change scenario:**
1. Table rebuilds (drops data)
2. Incremental logic ‚Üí only 5-10 min API fetch
3. **Data loss for > 2 days old data** ‚ùå

**Recovery:**
- Option A: Restore from backup (5-10 min)
- Option B: Delta time travel (if < 30 days)
- Option C: Set `one_time_load: true` and reload from API (4-5 hours)

---

## üîÑ Incremental Update Logic

### Configuration Parameters

```yaml
configuration:
  one_time_load: "false"     # Incremental mode (daily runs)
  lookback_days: "1"         # Safety buffer for late-arriving data
```

### Incremental Date Calculation

```python
def calculate_date_range():
    """
    Smart date range calculation for incremental processing.
    """
    one_time_load = spark.conf.get("one_time_load", "false").lower() == "true"
    lookback_days = int(spark.conf.get("lookback_days", "1"))
    today = date.today()
    
    if one_time_load:
        # Full historical load (first run or recovery)
        start_date = date(2023, 10, 1)
        end_date = today
        print(f"üìÖ FULL LOAD: {start_date} to {end_date} (~850 days)")
        print(f"   Estimated runtime: 4-5 hours")
    else:
        # Incremental load (daily runs)
        try:
            # Query max date from bronze table (OUTSIDE pipeline execution)
            max_date_result = spark.sql("""
                SELECT MAX(gameDate) as max_date 
                FROM lr_nhl_demo.dev.bronze_player_game_stats_v2
            """).first()
            
            if max_date_result and max_date_result["max_date"]:
                # Convert YYYYMMDD int to date
                max_date_str = str(max_date_result["max_date"])
                max_date = datetime.strptime(max_date_str, "%Y%m%d").date()
                
                # Start from max_date - lookback (safety buffer)
                start_date = max_date - timedelta(days=lookback_days)
                end_date = today
                
                print(f"üìÖ INCREMENTAL: {start_date} to {end_date}")
                print(f"   Last processed: {max_date}")
                print(f"   New dates: {(end_date - start_date).days} days")
                print(f"   Estimated runtime: 5-10 minutes")
            else:
                # Table exists but empty
                raise Exception("Table empty")
        except:
            # Table doesn't exist - use fallback
            start_date = today - timedelta(days=lookback_days)
            end_date = today
            print(f"üìÖ FALLBACK: {start_date} to {end_date}")
    
    return start_date, end_date
```

**Key Points:**
- Query happens ONCE at function start (not during iteration)
- Lookback buffer prevents missing late-arriving data
- Graceful fallback if table doesn't exist

---

## üóìÔ∏è Schedule Ingestion (Critical for Predictions!)

### Current Issue
**Problem:** Schedule derived ONLY from `games_historical` (completed games)
- Historical games: ‚úÖ Has data
- **Future games:** ‚ùå Missing!
- **Result:** No upcoming games for ML predictions

### Solution: Hybrid Approach

```python
@dlt.table(name="bronze_schedule_2023_v2")
def ingest_schedule_v2():
    """
    Combines historical + future schedule for complete coverage.
    """
    
    # PART 1: Historical schedule (from completed games)
    games_df = dlt.read("bronze_games_historical_v2")
    historical_schedule = derive_schedule_from_games(games_df)
    # Result: 3,955 historical games ‚úÖ
    
    # PART 2: Future schedule (from NHL schedule API)
    future_games = []
    for days_ahead in range(0, 8):  # Next 7 days
        date_str = (datetime.now().date() + timedelta(days=days_ahead)).strftime("%Y-%m-%d")
        
        schedule = nhl_client.schedule.daily_schedule(date=date_str)
        if schedule and "games" in schedule:
            for game in schedule["games"]:
                future_games.append({
                    "GAME_ID": game["id"],
                    "DATE": game_date,
                    "AWAY": game["awayTeam"]["abbrev"],
                    "HOME": game["homeTeam"]["abbrev"],
                    # ... other fields
                })
    
    future_schedule = spark.createDataFrame(future_games, schema=schedule_schema)
    # Result: 20-30 upcoming games ‚úÖ
    
    # PART 3: Combine
    combined = historical_schedule.unionByName(future_schedule)
    return combined.dropDuplicates(["GAME_ID"])
    # Result: 3,975 total games (historical + future) ‚úÖ
```

**Why This Works:**
- Historical games: Complete stats available
- Future games: Schedule known, waiting for stats
- Gold layer joins future schedule with player index ‚Üí predictions!

---

## üìä Bronze Table Inventory

| Table Name | Type | Source | Size | Update Freq | Purpose |
|------------|------|--------|------|-------------|---------|
| `bronze_player_game_stats_v2` | @dlt.table() | NHL API | 492K | Daily | Player-game stats |
| `bronze_games_historical_v2` | @dlt.table() | NHL API | 31K | Daily | Team-game stats |
| `bronze_schedule_2023_v2` | @dlt.table() | API + Derived | 3.9K | Daily | Full schedule (past + future) |
| `bronze_skaters_2023_v2` | @dlt.table() | Derived | 14K | Daily | Aggregated player stats |

**All tables use:**
- `pipelines.reset.allowed: "false"` (manual protection)
- Smart incremental date logic (fast rebuilds)
- Deduplication on write (data quality)

---

## üöÄ Migration Plan (Using Backups)

### Phase 1: Pre-Migration (DONE ‚úÖ)
- [x] Create backups of all bronze tables
- [x] Validate backups have correct record counts

### Phase 2: Clean Deployment (DO THIS)

#### Step 1: Drop Existing Tables
```sql
-- Drop ALL bronze tables for clean migration
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_schedule_2023_v2;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_skaters_2023_v2;

-- Drop any staging tables from previous attempts
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2_staging;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2_staging;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_player_game_stats_v2_staging_manual;
DROP TABLE IF EXISTS lr_nhl_demo.dev.bronze_games_historical_v2_staging_manual;
```

#### Step 2: Load Backups into Bronze Tables
```sql
-- Restore historical data from backups (fast!)
CREATE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2 AS
SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_backup_latest;

CREATE TABLE lr_nhl_demo.dev.bronze_games_historical_v2 AS
SELECT * FROM lr_nhl_demo.dev.bronze_games_historical_v2_backup_latest;

-- Schedule and skaters will be derived during pipeline run
```

**This gives you:**
- Immediate historical data restoration (5 minutes)
- DLT can then manage these tables going forward

#### Step 3: Deploy Clean Code
```yaml
# NHLPlayerIngestion.yml
configuration:
  one_time_load: "false"     # Incremental mode (historical data already in tables)
  lookback_days: "1"         # Process yesterday + today (safety buffer)
```

```bash
cd "/Users/logan.rupert/Library/CloudStorage/GoogleDrive-logan.rupert@databricks.com/My Drive/Repositories/NHL_SOG/nhlPredict"
databricks bundle deploy --profile e2-demo-field-eng
```

#### Step 4: Run Pipeline (Incremental)
Run via UI - should:
- ‚úÖ See existing historical data (492K, 31K records)
- ‚úÖ Fetch only recent data from API (5-10 min)
- ‚úÖ Fetch future schedule (next 7 days)
- ‚úÖ Derive schedule + skaters
- ‚úÖ Complete in 10-15 minutes

#### Step 5: Validate
```sql
-- Run: cursor_docs/QUICK_ML_VALIDATION.sql
-- Expect:
-- ‚úÖ Historical: 123,503+ records
-- ‚úÖ Upcoming: 300-500 records (NEW!)
-- ‚úÖ All features present
```

---

## üß™ Testing & Validation

### Test 1: Incremental Append Behavior
**Goal:** Verify only new data is fetched

```bash
# Run 1: Should fetch yesterday + today (~10-20 games)
databricks pipelines start

# Wait 24 hours

# Run 2: Should fetch today + tomorrow (~10-15 games)
databricks pipelines start

# Validate: Record count should increase by ~1,800 records
```

**Expected:**
- Day 1: 492,572 ‚Üí 494,300 (+ ~1,800)
- Day 2: 494,300 ‚Üí 496,100 (+ ~1,800)

### Test 2: Code Change Safety (After Backup)
**Goal:** Verify rebuild is fast even if data loss occurs

```python
# Make trivial code change
@dlt.table(name="bronze_player_game_stats_v2")
def ingest_player_game_stats_v2():
    # TEST COMMENT: Added to trigger rebuild
    ...
```

**Deploy and run:**
- ‚ö†Ô∏è Table will rebuild (data loss)
- ‚úÖ But only fetches last 2 days from API (5-10 min)
- ‚úÖ Can restore from backup in 5-10 min

**This demonstrates:**
- Rebuilds are fast (not 4-5 hours)
- Recovery is fast (backups + time travel)
- Acceptable trade-off for simplicity

### Test 3: Backup Recovery
**Goal:** Verify backup restore works

```sql
-- Simulate data loss
DROP TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2;

-- Restore from backup (should take 5-10 minutes)
CREATE TABLE lr_nhl_demo.dev.bronze_player_game_stats_v2 AS
SELECT * FROM lr_nhl_demo.dev.bronze_player_game_stats_v2_backup_latest;

-- Validate
SELECT COUNT(*) FROM lr_nhl_demo.dev.bronze_player_game_stats_v2;
-- Expected: 492,572
```

---

## üìù Implementation Checklist

### Code Changes Required

#### 1. Bronze Ingestion (01-bronze-ingestion-nhl-api.py)

**Remove:**
- [ ] All `dlt.create_streaming_table()` definitions
- [ ] All `@dlt.append_flow()` decorators
- [ ] `load_from_backup` parameter logic
- [ ] `skip_staging_ingestion` parameter logic
- [ ] Staging table functions (`_staging`, `_staging_manual`)
- [ ] `spark.readStream` conversions

**Keep/Add:**
- [ ] Simple `@dlt.table()` decorators for all 4 tables
- [ ] Smart incremental date logic (already working)
- [ ] Deduplication (already working)
- [ ] `pipelines.reset.allowed: "false"` in table_properties
- [ ] **ADD:** Future schedule fetching in `ingest_schedule_v2()`

**Result:** ~200 fewer lines of code, much simpler!

#### 2. Schedule Ingestion (CRITICAL FIX)

**Current (Broken):**
```python
def ingest_schedule_v2():
    games_df = dlt.read("bronze_games_historical_v2")  # Only past!
    return derive_schedule(games_df)  # No future games ‚ùå
```

**Fixed:**
```python
def ingest_schedule_v2():
    # Part 1: Historical from games
    historical = derive_schedule_from_games()
    
    # Part 2: Future from API (CRITICAL!)
    future = fetch_future_schedule_from_api(next_7_days)
    
    # Part 3: Combine
    return historical.union(future).dropDuplicates(["GAME_ID"])
```

#### 3. Configuration (NHLPlayerIngestion.yml)

**Remove:**
- [ ] `skip_staging_ingestion` parameter (no longer needed)
- [ ] `load_from_backup` parameter (no longer needed)

**Keep:**
- [x] `one_time_load: "false"` (incremental mode)
- [x] `lookback_days: "1"` (safety buffer)
- [x] `notifications` (email alerts)

---

## üìä Storage & Cost Analysis

### Option A (Simplified - Recommended)
| Component | Storage | Cost/Month |
|-----------|---------|------------|
| Bronze tables (4) | ~1 GB | ~$0.50 |
| Silver tables (3) | ~2 GB | ~$1.00 |
| Gold tables (4) | ~3 GB | ~$1.50 |
| Backups (weekly) | ~1 GB | ~$0.50 |
| **Total** | **~7 GB** | **~$3.50** |

### Option B (Staging Pattern - Complex)
| Component | Storage | Cost/Month |
|-----------|---------|------------|
| Bronze staging (2) | ~1 GB | ~$0.50 |
| Bronze final (2) | ~1 GB | ~$0.50 |
| Bronze derived (2) | ~0.2 GB | ~$0.10 |
| Silver tables (3) | ~2 GB | ~$1.00 |
| Gold tables (4) | ~3 GB | ~$1.50 |
| Checkpoints | ~0.5 GB | ~$0.25 |
| Backups (weekly) | ~1 GB | ~$0.50 |
| **Total** | **~8.7 GB** | **~$4.35** |

**Savings with Option A:** ~$0.85/month (~20% less storage)

---

## üéØ Recommended Implementation Steps

### Step 1: Simplify Code (Remove Staging Pattern)
**Action:** Revert to simple `@dlt.table()` for all bronze tables
**Time:** 30 minutes (code changes)
**Risk:** Low (can restore from backups)

### Step 2: Fix Schedule for Future Games
**Action:** Add NHL schedule API fetch for next 7 days
**Time:** 15 minutes (code changes)
**Risk:** Low (just adding data, not changing existing)

### Step 3: Clean Migration
**Action:** Drop tables, restore from backups, deploy, run
**Time:** 30 minutes (execution + validation)
**Risk:** Low (have backups)

### Step 4: Validate ML Readiness
**Action:** Run QUICK_ML_VALIDATION.sql
**Expected:** 300-500 upcoming games for predictions ‚úÖ
**Time:** 5 minutes

### Step 5: Test Incremental Run
**Action:** Run pipeline again next day
**Expected:** Fast execution (5-10 min), data appends ‚úÖ
**Time:** 10 minutes

### Step 6: Implement Backup Automation (Future)
**Action:** Create weekly backup job
**Time:** 30 minutes
**Risk:** None (safety improvement)

**Total implementation time:** ~2 hours

---

## ‚ö†Ô∏è Lessons Learned from Previous Attempts

### What Didn't Work

1. **`@dlt.append_flow()` with batch API data**
   - Error: "Cannot create append flow from batch query"
   - Root cause: Batch DataFrame != Streaming DataFrame
   - Workaround: Staging pattern (too complex)

2. **`pipelines.reset.allowed: "false"` alone**
   - Doesn't prevent code-triggered rebuilds with `@dlt.table()`
   - Only prevents manual resets via UI/API

3. **Streaming table ownership conflicts**
   - Manual `CREATE TABLE` ‚Üí DLT tries to manage ‚Üí ownership error
   - DLT must create tables OR you use different pipeline

4. **Schedule derived from games_historical only**
   - Missing future games ‚Üí no ML predictions possible
   - Must fetch future schedule from NHL API

### What Works

1. **Smart incremental date logic**
   - Even if table rebuilds, only fetches recent data
   - Makes rebuilds fast (5-10 min vs 4-5 hours)

2. **Regular backups + time travel**
   - Fast recovery from any data loss scenario
   - 30-day Delta retention + long-term backups

3. **Deduplication on write**
   - Prevents data quality issues downstream
   - Simple and effective

4. **`one_time_load` parameter**
   - Easy toggle between full and incremental modes
   - Clear operational model

---

## üèÜ Success Criteria

### Bronze Layer
- ‚úÖ 492K+ player-game records (historical)
- ‚úÖ 31K+ team-game records (historical)
- ‚úÖ 3,975+ schedule records **(historical + 7 days future)**
- ‚úÖ 14K+ skater aggregations
- ‚úÖ Fast incremental runs (5-10 minutes)
- ‚úÖ No data loss on normal operations

### Gold Layer (ML Ready)
- ‚úÖ 123K+ historical records (training data)
- ‚úÖ **300-500 upcoming game records (prediction targets)**
- ‚úÖ All playerIds populated (no nulls)
- ‚úÖ All rolling features populated
- ‚úÖ Target variable (SOG) = NULL for upcoming games
- ‚úÖ No duplicates

### Operational
- ‚úÖ Daily runs complete in 5-10 minutes
- ‚úÖ Code changes don't require 4-5 hour reloads
- ‚úÖ Backup recovery in 5-10 minutes
- ‚úÖ Email notifications on success/failure
- ‚úÖ Simple architecture (easy to maintain)

---

## üìñ References & Best Practices

### Databricks Official Guidance

**For Batch API Ingestion:**
> "Use `@dlt.table()` with deduplication for batch API sources. Reserve streaming tables for truly streaming sources (Kafka, Kinesis, Auto Loader)."
> ‚Äî [DLT Best Practices](https://docs.databricks.com/en/delta-live-tables/best-practices.html)

**For Incremental Processing:**
> "Implement date-based filtering in your ingestion logic. Query the target table to determine last processed date, then fetch only new data."
> ‚Äî [DLT Incremental Processing](https://docs.databricks.com/en/delta-live-tables/incremental.html)

**For Data Protection:**
> "Use `pipelines.reset.allowed: false` to prevent accidental full refreshes. Combine with regular backups for complete protection."
> ‚Äî [DLT Table Properties](https://docs.databricks.com/en/delta-live-tables/properties.html)

### Industry Best Practices (Medallion Architecture)

**Bronze Layer:**
- Raw data, minimal transformation
- Idempotent writes (deduplication)
- Incremental ingestion preferred
- **Use simple patterns** for batch sources

**Silver Layer:**
- Cleaned, conformed data
- Business logic applied
- Can use more complex patterns

**Gold Layer:**
- Aggregated, ML-ready features
- Optimized for query performance
- May have multiple gold tables for different use cases

---

## üéØ Final Recommendation

**Architecture:** Option A - Simplified `@dlt.table()` with Smart Incremental Logic

**Rationale:**
1. **Proven pattern:** Used in 90% of production DLT pipelines
2. **Simple:** 50% less code than staging pattern
3. **Fast:** 5-10 min incremental + 5-10 min rebuilds
4. **Safe:** Backups + time travel provide adequate protection
5. **Cost-effective:** 20% less storage
6. **Maintainable:** Easy to understand and debug

**Protection Strategy:**
- Daily incremental runs (no issues)
- Code changes trigger fast rebuilds (5-10 min)
- Weekly automated backups for long-term recovery
- Delta time travel for 30-day recovery window

**Key Fix Needed:**
- Add future schedule fetching to `bronze_schedule_2023_v2`
- This unlocks 300-500 upcoming game predictions!

---

## üöÄ Next Steps

1. **Review this architecture plan** - confirm it meets your requirements
2. **Approve simplified approach** - remove staging pattern complexity
3. **Implement schedule fix** - add future game fetching (15 min)
4. **Clean migration** - drop tables, restore backups, deploy (30 min)
5. **Validate** - verify ML predictions have upcoming games (5 min)
6. **Test** - run incremental pipeline, verify fast execution (10 min)
7. **Done!** - Production-ready architecture ‚úÖ

**Total time to working solution:** ~1 hour

---

## üìû Support & Maintenance

### Daily Operations
```bash
# Standard daily run (automated via job scheduler)
databricks pipelines start-update --pipeline-id <ID>
# Runtime: 5-10 minutes
```

### Weekly Backup
```sql
-- Automated via Databricks job (weekly)
CREATE OR REPLACE TABLE bronze_player_game_stats_v2_backup_latest
DEEP CLONE bronze_player_game_stats_v2;

CREATE OR REPLACE TABLE bronze_games_historical_v2_backup_latest
DEEP CLONE bronze_games_historical_v2;
```

### Recovery from Code Change
```sql
-- Option 1: Restore from backup (5-10 min)
CREATE OR REPLACE TABLE bronze_player_game_stats_v2 AS
SELECT * FROM bronze_player_game_stats_v2_backup_latest;

-- Option 2: Time travel (if < 30 days ago)
CREATE OR REPLACE TABLE bronze_player_game_stats_v2 AS
SELECT * FROM bronze_player_game_stats_v2 VERSION AS OF <version>;

-- Option 3: Full reload from API (4-5 hours)
# Set one_time_load: "true", deploy, run
```

### Monitoring
- Email notifications on pipeline success/failure
- Dashboard with data freshness metrics
- Automated validation queries

---

**Ready to implement this simplified, best-practice architecture?** üöÄ
